{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nRo5JCdHtI7v",
    "outputId": "aea4effb-1243-4b1b-f806-edfaa2f418aa"
   },
   "outputs": [],
   "source": [
    "!pip install -q --upgrade llama-index transformers accelerate bitsandbytes\n",
    "!pip install -q llama-index-llms-huggingface llama-index-embeddings-huggingface\n",
    "!pip install -q chromadb llama-index-vector-stores-chroma\n",
    "!pip install -q sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uHxbHVFacMT3",
    "outputId": "9ba25dab-1779-44d3-a2d4-faea51ce5d42"
   },
   "outputs": [],
   "source": [
    "!pip -q install llama-index-retrievers-bm25\n",
    "!pip -q install llama-index-core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "62uMd__cUDyN",
    "outputId": "d66a64ea-b524-4bd0-bacb-f25d3c99b13e"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rA7DDHGIUFJs",
    "outputId": "cb3d3232-b34c-4abb-abdf-b8a2ec60e55e"
   },
   "outputs": [],
   "source": [
    "!hf auth login\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iZ-CtySXUGeV"
   },
   "outputs": [],
   "source": [
    "DOCX_PATH = \"/content/drive/MyDrive/DTCNTT/DMS-5.pdf\"\n",
    "CACHE_FILE = \"/content/drive/MyDrive/DTCNTT/data/cache/pipeline_cache.json\"\n",
    "INDEX_STORAGE = \"/content/drive/MyDrive/DTCNTT/data/index_store_V6_maximalist\"\n",
    "COLLECTION_NAME = \"dsm5_V6_store\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gBZc10wBUG9C"
   },
   "outputs": [],
   "source": [
    "# --- C√°c th∆∞ vi·ªán h·ªá th·ªëng & ti·ªán √≠ch ---\n",
    "import traceback\n",
    "import gc\n",
    "import torch\n",
    "import chromadb\n",
    "torch.cuda.empty_cache()\n",
    "# --- Hugging Face & Sentence Transformers ---\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from sentence_transformers import CrossEncoder\n",
    "\n",
    "# --- LlamaIndex Core & LLM ---\n",
    "from llama_index.core import Settings, VectorStoreIndex, StorageContext\n",
    "from llama_index.core.schema import TextNode\n",
    "from llama_index.core.llms import ChatMessage\n",
    "from llama_index.core.memory import ChatMemoryBuffer\n",
    "\n",
    "# --- LlamaIndex Retrievers & Storage ---\n",
    "from llama_index.llms.huggingface import HuggingFaceLLM\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "from llama_index.retrievers.bm25 import BM25Retriever\n",
    "from llama_index.core.retrievers import VectorIndexRetriever, QueryFusionRetriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NVQ4RZfurP5F",
    "outputId": "44ed77ef-89c0-48e9-e814-6bfdd608cf5c"
   },
   "outputs": [],
   "source": [
    "!nvidia-smi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7UPxuOOiUSs3"
   },
   "outputs": [],
   "source": [
    "def setup_llm_and_embedding():\n",
    "    global embed_model, llm, model\n",
    "\n",
    "    # D·ªçn VRAM\n",
    "    print(\"üßπ Clearing VRAM...\")\n",
    "    for obj in [\"llm\", \"model\", \"embed_model\"]:\n",
    "        if obj in globals():\n",
    "            del globals()[obj]\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"‚úÖ VRAM cleared.\\n\")\n",
    "\n",
    "    # ------------------------------\n",
    "    # 1. Load Embedding\n",
    "    # ------------------------------\n",
    "    print(\"‚è≥ Loading embedding...\")\n",
    "    embed_model = HuggingFaceEmbedding(\n",
    "        model_name=\"AITeamVN/Vietnamese_Embedding\",\n",
    "        device=\"cpu\"\n",
    "    )\n",
    "    Settings.embed_model = embed_model\n",
    "    print(\"‚úÖ Embedding ready.\\n\")\n",
    "\n",
    "    # ------------------------------\n",
    "    # 2. Load Tokenizer t·ª´ HF HUB\n",
    "    # ------------------------------\n",
    "    REPO_ID = \"letri345/llama3-8b-merge\"  # <‚Äî s·ª≠a t√™n repo ·ªü ƒë√¢y\n",
    "\n",
    "    print(f\"‚è≥ Loading tokenizer from {REPO_ID}...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(REPO_ID, trust_remote_code=True)\n",
    "\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    # Gi·ªØ template chat ƒë√£ fine-tune\n",
    "    tokenizer.chat_template = (\n",
    "        \"<|begin_of_text|>\"\n",
    "        \"{% for message in messages %}\"\n",
    "        \"{% if message['role'] == 'user' %}\"\n",
    "            \"<|start_header_id|>user<|end_header_id|>\\n\\n{{ message['content'] }}<|eot_id|>\"\n",
    "        \"{% elif message['role'] == 'system' %}\"\n",
    "            \"<|start_header_id|>system<|end_header_id|>\\n\\n{{ message['content'] }}<|eot_id|>\"\n",
    "        \"{% elif message['role'] == 'assistant' %}\"\n",
    "            \"<|start_header_id|>assistant<|end_header_id|>\\n\\n{{ message['content'] }}<|eot_id|>\"\n",
    "        \"{% endif %}\"\n",
    "        \"{% endfor %}\"\n",
    "    )\n",
    "\n",
    "    # ------------------------------\n",
    "    # 3. Load Model 4-bit t·ª´ HF HUB\n",
    "    # ------------------------------\n",
    "    print(\"‚ö° Loading 4-bit model from HuggingFace Hub...\")\n",
    "\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "    )\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        REPO_ID,\n",
    "        device_map=\"auto\",\n",
    "        quantization_config=bnb_config,\n",
    "        trust_remote_code=True,\n",
    "    )\n",
    "\n",
    "    print(\"‚úÖ Model loaded successfully (4-bit).\")\n",
    "\n",
    "    # ------------------------------\n",
    "    # 4. Wrap cho LlamaIndex\n",
    "    # ------------------------------\n",
    "    eos_id = tokenizer.eos_token_id\n",
    "    eot_id = tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "\n",
    "    llm = HuggingFaceLLM(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        context_window=8192,\n",
    "        max_new_tokens=512,\n",
    "        generate_kwargs={\"temperature\": 0.3},\n",
    "        stopping_ids=[eos_id, eot_id],\n",
    "    )\n",
    "\n",
    "    Settings.llm = llm\n",
    "    print(\"\\nüéâ Llama 3 8B ƒë√£ s·∫µn s√†ng s·ª≠ d·ª•ng t·ª´ HuggingFace Hub!\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 785,
     "referenced_widgets": [
      "7cc54e4fa1f2439895224c89d1cab83c",
      "08b1b3d3f36d4c90bb9aa8daace023aa",
      "d4e1da5c346b4957a953f6a67394bff6",
      "cc11b92f740f4d3d940dc38862788f2d",
      "6a4d5f84b7c24892b3d46849a83dc2c2",
      "b3cf8e7bd1c64b0c8f99ea2d503684a0",
      "fa650f930cc74235bf77840be064451a",
      "e8021c0cc5514d7dae6e41b15b33dd4f",
      "0b17bc422272475a92016ba2f51348dc",
      "64367d88d2a648699219c0de691e2401",
      "f1110380199042c9be0af00a02982207",
      "5f3283d06b0c4e92817c1badd1fc2ca3",
      "5747a9225d2340019c1f8963229c6f93",
      "d1f9aafe624e44e495cd859fdb3465cd",
      "47a6e372f6d34ef98fae65c63d4ab092",
      "7765c6797a144da0ac4d0b1fb4232979",
      "4ff5b7c0e4d74488a98297fbfc2d3fd2",
      "bf95f1a858b34de7852a6d567638e8de",
      "77924da4f288419bab6a795cf0737dec",
      "edcfab99579e4e75a20fa0f5e10fdcb2",
      "2705dd2df64a4713a4c323dd30a4a809",
      "bd71f2ff041948c68d809c31f733eea8",
      "2896acda972e415ba91da1bd601a9634",
      "fdf0d25ae620440c9c3022cca66cda21",
      "1ea07196caf24ae59af6c061a33693e7",
      "380d2ced693d48b49ea037133839ae82",
      "7d4bdd9590e943da8b49d5c9f3d56b57",
      "b51a005001584dc6ba75f5c314d81097",
      "a2c54511e3e245b0817c0adb486959df",
      "8600c4c8819f44a7b12801a0aed21838",
      "c678580ffaa448c586be648c67e130e0",
      "6a1ad24ee85c418784fdb44f93ad651c",
      "923dd8149ae8415295484936f5d15bf6",
      "458bc06c872745d2ab08e5ca3f872620",
      "af1914fbdd4b4a16b3a924062e961ee4",
      "7c7446dd848449ffbfc5df47eed49091",
      "483515b8a4b7433a9a43fa30c9fb0a2a",
      "55e7d53f5cd7441782fba4944c91e277",
      "12d6eabc6b714a079b641ab5ce394db0",
      "6887a0a9d36743039dd1512977297fe3",
      "eb2245645380467f9561b77659bcca89",
      "4661bcfe8ed34f06968722d387d64878",
      "a60b87e5b1f34862abeeb55ae818fde5",
      "64e73cb1002a4042adfc192c72e836d2",
      "0e93ca30baa443cfb5f623edc34c9b51",
      "6bd93916d7264692bb7494bda8d65c65",
      "116b74416beb4f6fb2c7fdb40d925b8b",
      "b1d7898fce9f4549b65f8c81d4a58ddb",
      "61fdc2f3d80649b88a8c0b95229d76ab",
      "9be1d32098af4ebe8578d6d2c4dc4666",
      "651e2bf8c56e4a0aae204082f6ed06f2",
      "07f7b70288e24b32a29db65430b93e2f",
      "eef93619d7b149d58a077b220367fb12",
      "e1b1eb69da534b04a2e4c9a82d87b525",
      "e4cea3e035b54fb3aa33a0b29dd43acb",
      "bc6a4470c1f34bdfaa61b44d6b2c741e",
      "91909b7f7fb54adcbd8ebb237fb5d383",
      "380cb33f9926403ca4051f09fbe136b6",
      "e9ff136974384e0b8f58868af78fb46d",
      "b4f129c70d71447a859f76d5ca703af8",
      "6e5c82e61d844458af5ecf374e1c99d9",
      "d77606dc6e4b47419574ccf7406884fc",
      "f1e384e864de4faa88c63b9429afc596",
      "4fd54cdf8de848aa914953f86d6aebfd",
      "3922fb9fc6f84797aacecc5a101dc037",
      "2a6c2592be694672aa2cb6d68a032098",
      "bcebec2a223b491abb8bdc6a0a0ebdbc",
      "56668bb2a5f54663a73ba9a3551699e1",
      "746840ce22ff43f08e44bdb13266538a",
      "0c3971c0912b43728527155ea57be637",
      "f309b88a348d496dbeb9e67a60f28df5",
      "ac5318659f674c3599bcdafbebecb657",
      "037b5173303f4607a48df8ce00a80c4f",
      "46b0ea487a09470ba4549fcf99ecb88a",
      "2f28e18ea6eb4575b96a3a07a245d2a7",
      "c49e85d21abb4d97bd9b88726a4f5d88",
      "df626c97439240558a2907a0c01c7a32",
      "5aac3b356aef4da5a7820258d6a07119",
      "6e6733c71ed74583b9ece1d83b055ebe",
      "0a82b1a73610412fb63808eb07e8d272",
      "fe01cdae488d49a4920ef4cb45b1f619",
      "9fcb22edacfa499dac6db6c0e3506e95",
      "aa9a470dd7e2479ca549ea84da0ae545",
      "7481b47e6f3e4ad2a9abb6101d2c162c",
      "237c19ca7b804fd886c4f05231cde570",
      "e1c7f1da9a394726aabb3d04b4be3d19",
      "3d6569ee9ff342c9b9a8b3ccd5e2e5be",
      "007e0fef31954f4a942f41944ec55fc6",
      "3e703412c9274ec7b800a348e57886f8",
      "0d90c2947c154722805e6aa6bd74d478",
      "7ae2ca3a1d9940cf989f610de3886776",
      "b7b18321212641bc994db008ad140307",
      "6aa778f41ba6420c97fed52f21c17f09",
      "9bd5e0978e474e71ab22f8c2954b7179",
      "0a3b1901a942441eb35da2abc42476dc",
      "45113de722bb440db884d0a98798f629",
      "a76b85a179034160b1426ed7d8eee1a4",
      "6eb76087b6ad453b9d216bcdc018fd4d",
      "5a3a0d6669c849fa8158cf48c21cae9a",
      "3e2c6f5870dc4ef19e3fd748711e7703",
      "e08c147339be4c7394128a317a7701e2",
      "2ec451f49b574df49ad73d54b06aa931",
      "15056a036bb049f1875baf0dff95e909",
      "a8717a0d753d4a50bd9a242460dae1d4",
      "88d789e7437e437ea70ef972a389e14b",
      "d46b281f97774650a8ea1c10332e0025",
      "36538a0916b74221b40a64609e0fdb3f",
      "40b0e378b1534a08aac6cc97f08adc83",
      "593a1b532dff440faddbba2937be413d",
      "45b567e22ff44fca933f2d30d7e7ecee",
      "5d1727b23e9840808cd034e35e966303",
      "581424213acb49dab6f8a1e49925445e",
      "4ad0f478e6fc4309865bdfcc38fc4e01",
      "7c0a65c1628c452a9742adfdb0a485b2",
      "d3ef4029e73942379b213dc7e65e3e75",
      "ef2ffa0928574576b452aea9c5c53604",
      "695a362e2acf4f71b9e20a85929bf5b5",
      "40b4ea7929254c87a0bfa0729919dadb",
      "ccdab60f577e44e69ce58c0fa0a0173e",
      "85e60d66a9394cb4bbee2ee2884d32d7",
      "7a347fc0deec4452b3d39d9d0955dbd2",
      "a1c322a2507145f08ff1dee187d73de8",
      "3fdd2bd3face499d962d79a5900d86fd",
      "0f4240a355534ce0aca98d42cb29c0ac",
      "db15df35f695467ca40707bae136cacf",
      "9c09bdacf3ef4fc99f843c3211d0de26",
      "683ebdd43c834e668227b3b17b4d5e12",
      "395c2f6966d34c1c8ea0c511e8dc49d7",
      "1a816fc55a1442bea8c686226b1f3053",
      "767019ad7a5741b195cd7d64b60e8a6a",
      "6a36f2054aa64c1180e068ef508ecbaa",
      "58a537d248a64d1496544d4a78121c03"
     ]
    },
    "id": "qOGIXKR6cbWX",
    "outputId": "f2288bd3-bf48-43c3-e48b-4277f6afd0e7"
   },
   "outputs": [],
   "source": [
    "setup_llm_and_embedding()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iXfm2UdczY2V",
    "outputId": "d1a25281-e689-41b2-dc66-5f062e00ff03"
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ihu1owzl3FUc"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4lIikNPXdcxT",
    "outputId": "be85d9bb-dc9b-47e4-f045-d8cd88700cb0"
   },
   "outputs": [],
   "source": [
    "db = chromadb.PersistentClient(path=INDEX_STORAGE)\n",
    "col = db.get_collection(COLLECTION_NAME)\n",
    "print(\"üì¶ S·ªë l∆∞·ª£ng vectors trong Chroma:\", col.count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nHVVlHG4xQq9"
   },
   "outputs": [],
   "source": [
    "def print_page(page_number):\n",
    "    print(f\"\\n==================== PAGE {page_number} ====================\\n\")\n",
    "\n",
    "    db = chromadb.PersistentClient(path=INDEX_STORAGE)\n",
    "    col = db.get_collection(COLLECTION_NAME)\n",
    "\n",
    "    results = col.get(\n",
    "        where={\"page_label\": str(page_number)},\n",
    "        include=[\"documents\", \"metadatas\"]\n",
    "    )\n",
    "\n",
    "    if not results[\"documents\"]:\n",
    "        print(\"‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y trang n√†y trong Chroma.\")\n",
    "        return\n",
    "\n",
    "    text = results[\"documents\"][0]\n",
    "    meta = results[\"metadatas\"][0]\n",
    "\n",
    "    # In text\n",
    "    print(\"üìÑ **N·ªòI DUNG TRANG:**\")\n",
    "    print(\"-\" * 60)\n",
    "    print(text[:1500])\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "    # In metadata\n",
    "    print(\"\\nüîñ **METADATA:**\")\n",
    "    for key, val in meta.items():\n",
    "        print(f\"  ‚Ä¢ {key}: {val}\")\n",
    "\n",
    "    # üî¢ Th√™m ph·∫ßn ƒë·∫øm ƒë·ªô d√†i\n",
    "    print(\"\\nüî¢ **ƒê·ªò D√ÄI DOCUMENT:**\")\n",
    "    print(f\"  ‚Ä¢ S·ªë k√Ω t·ª±: {len(text)}\")\n",
    "    print(f\"  ‚Ä¢ S·ªë t·ª´: {len(text.split())}\")\n",
    "\n",
    "    print(\"\\n============================================================\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nhrGDfKjxpmA",
    "outputId": "a38c48bd-7002-4126-d766-99798a43fa6f"
   },
   "outputs": [],
   "source": [
    "print_page(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H9f2GGt9UYge"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def should_use_rag(message: str, last_user_message: str = \"\") -> bool:\n",
    "    print(\"üß† [Gate] ƒêang ph√¢n t√≠ch √Ω ƒë·ªãnh (Expanded)...\")\n",
    "\n",
    "    msg = message.lower().strip()\n",
    "    last_msg = last_user_message.lower().strip()\n",
    "\n",
    "    # =================================================================\n",
    "    # üö® NH√ìM 0: KH·ª¶NG HO·∫¢NG & AN TO√ÄN (CRISIS)\n",
    "    # C√°c t·ª´ kh√≥a n√†y b√°o hi·ªáu nguy hi·ªÉm -> C·∫ßn x·ª≠ l√Ω ngay (Force RAG ho·∫∑c Alert)\n",
    "    # =================================================================\n",
    "    crisis_keywords = [\n",
    "        \"t·ª± t·ª≠\", \"t·ª± s√°t\", \"suicidal\", \"mu·ªën ch·∫øt\", \"k·∫øt th√∫c cu·ªôc ƒë·ªùi\",\n",
    "        \"r·∫°ch tay\", \"t·ª± h·∫°i\", \"self-harm\", \"nh·∫£y l·∫ßu\", \"u·ªëng thu·ªëc ng·ªß\",\n",
    "        \"gi·∫øt ng∆∞·ªùi\", \"h·∫°i ng∆∞·ªùi\", \"kh√¥ng mu·ªën s·ªëng n·ªØa\", \"tuy·ªát v·ªçng\"\n",
    "    ]\n",
    "\n",
    "    if any(k in msg for k in crisis_keywords):\n",
    "        print(\"üß† [Gate] ‚ö†Ô∏è PH√ÅT HI·ªÜN T√çN HI·ªÜU KH·∫®N C·∫§P.\")\n",
    "        return True\n",
    "\n",
    "    # =================================================================\n",
    "    # üìö NH√ìM 1: T√äN R·ªêI LO·∫†N (DISORDERS) - M·ªü r·ªông theo ch∆∞∆°ng DSM-5\n",
    "    # =================================================================\n",
    "    disorders = [\n",
    "        # --- T√¢m tr·∫°ng & Lo √¢u ---\n",
    "        \"tr·∫ßm c·∫£m\", \"depression\", \"u u·∫•t\", \"kh√≠ s·∫Øc\",\n",
    "        \"l∆∞·ª°ng c·ª±c\", \"bipolar\", \"h∆∞ng c·∫£m\", \"mania\", \"hypomania\",\n",
    "        \"lo √¢u\", \"anxiety\", \"ho·∫£ng lo·∫°n\", \"panic\", \"s·ª£ x√£ h·ªôi\", \"√°m ·∫£nh s·ª£\",\n",
    "\n",
    "        # --- OCD & Stress ---\n",
    "        \"ocd\", \"√°m ·∫£nh c∆∞·ª°ng b·ª©c\", \"nghi th·ª©c\", \"t√≠ch tr·ªØ\", \"hoarding\",\n",
    "        \"ptsd\", \"sang ch·∫•n\", \"h·∫≠u ch·∫•n th∆∞∆°ng\", \"stress c·∫•p t√≠nh\", \"th√≠ch ·ª©ng\",\n",
    "\n",
    "        # --- Lo·∫°n th·∫ßn (Psychotic) ---\n",
    "        \"t√¢m th·∫ßn ph√¢n li·ªát\", \"schizophrenia\", \"hoang t∆∞·ªüng\", \"·∫£o gi√°c\",\n",
    "        \"nghe ti·∫øng n√≥i\", \"lo·∫°n th·∫ßn\", \"psychosis\", \"catatonia\", \"cƒÉng tr∆∞∆°ng l·ª±c\",\n",
    "\n",
    "        # --- Ph√°t tri·ªÉn th·∫ßn kinh ---\n",
    "        \"t·ª± k·ª∑\", \"autism\", \"asd\", \"tƒÉng ƒë·ªông\", \"gi·∫£m ch√∫ √Ω\", \"adhd\",\n",
    "        \"khi·∫øm khuy·∫øt tr√≠ tu·ªá\", \"ch·∫≠m ph√°t tri·ªÉn\", \"tik\", \"tourette\",\n",
    "\n",
    "        # --- ƒÇn u·ªëng (Feeding & Eating) ---\n",
    "        \"ch√°n ƒÉn\", \"anorexia\", \"ƒÉn v√¥ ƒë·ªô\", \"bulimia\", \"binge eating\", \"pica\",\n",
    "\n",
    "        # --- Nh√¢n c√°ch (Personality) ---\n",
    "        \"r·ªëi lo·∫°n nh√¢n c√°ch\", \"borderline\", \"ranh gi·ªõi\", \"√°i k·ª∑\", \"narcissistic\",\n",
    "        \"ch·ªëng ƒë·ªëi x√£ h·ªôi\", \"antisocial\", \"tr√°nh n√©\", \"ph·ª• thu·ªôc\", \"ƒëa nh√¢n c√°ch\",\n",
    "\n",
    "        # --- Gi·∫•c ng·ªß & Kh√°c ---\n",
    "        \"m·∫•t ng·ªß\", \"insomnia\", \"ng·ªß r≈©\", \"narcolepsy\", \"√°c m·ªông\",\n",
    "        \"nghi·ªán\", \"cai nghi·ªán\", \"ma t√∫y\", \"r∆∞·ª£u\", \"ch·∫•t k√≠ch th√≠ch\",\n",
    "        \"r·ªëi lo·∫°n t√¨nh d·ª•c\", \"lo·∫°n d·ª•c\", \"gi·ªõi t√≠nh\", \"dysphoria\",\n",
    "        \"m·∫•t tr√≠ nh·ªõ\", \"alzheimer\", \"gi·∫£ b·ªánh\"\n",
    "    ]\n",
    "\n",
    "    # =================================================================\n",
    "    # ü©∫ NH√ìM 2: TRI·ªÜU CH·ª®NG & BI·ªÇU HI·ªÜN (SYMPTOMS)\n",
    "    # Nh·ªØng t·ª´ m√¥ t·∫£ tr·∫°ng th√°i b·ªánh l√Ω c·ª• th·ªÉ\n",
    "    # =================================================================\n",
    "    symptoms = [\n",
    "        \"·∫£o thanh\", \"·∫£o ·∫£nh\", \"m·∫•t ki·ªÉm so√°t\", \"b·ªëc ƒë·ªìng\", \"v√¥ c·∫£m\",\n",
    "        \"k√≠ch ƒë·ªông\", \"g√¢y g·ªï\", \"thu m√¨nh\", \"s·ª£ ƒë√°m ƒë√¥ng\", \"r·ª≠a tay li√™n t·ª•c\",\n",
    "        \"ki·ªÉm tra li√™n t·ª•c\", \"nh·ªõ l·∫°i\", \"flashback\", \"√°c m·ªông\",\n",
    "        \"m·ªát m·ªèi kinh ni√™n\", \"s·ª•t c√¢n\", \"tƒÉng c√¢n\", \"m·∫•t ng·ªß k√©o d√†i\",\n",
    "        \"tim ƒë·∫≠p nhanh\", \"kh√≥ th·ªü\", \"ng·∫•t\", \"run tay\", \"v√£ m·ªì h√¥i\",\n",
    "        \"tr·ªëng r·ªóng\", \"b·ªè r∆°i\", \"ƒëa nghi\", \"ghen tu√¥ng hoang t∆∞·ªüng\"\n",
    "    ]\n",
    "\n",
    "    # =================================================================\n",
    "    # üîç NH√ìM 3: √ù ƒê·ªäNH TRA C·ª®U (DIAGNOSTIC INTENT)\n",
    "    # C√°c t·ª´ kh√≥a th·ªÉ hi·ªán user mu·ªën t√¨m ki·∫øn th·ª©c\n",
    "    # =================================================================\n",
    "    diagnostic_intent = [\n",
    "        \"ti√™u chu·∫©n\", \"d·∫•u hi·ªáu\", \"bi·ªÉu hi·ªán\", \"tri·ªáu ch·ª©ng\",\n",
    "        \"ch·∫©n ƒëo√°n\", \"diagnose\", \"criteria\", \"x√©t nghi·ªám\",\n",
    "        \"dsm\", \"dsm-5\", \"icd\", \"m√£ b·ªánh\",\n",
    "        \"c√≥ ph·∫£i l√†\", \"c√≥ ph·∫£i b·ªã\", \"t√¥i b·ªã g√¨\", \"b·ªánh g√¨\",\n",
    "        \"ph√¢n bi·ªát\", \"kh√°c nhau\", \"nguy√™n nh√¢n\", \"y·∫øu t·ªë nguy c∆°\",\n",
    "        \"th·ªùi gian k√©o d√†i\", \"bao l√¢u th√¨\", \"ti√™n l∆∞·ª£ng\"\n",
    "    ]\n",
    "\n",
    "    # --- LOGIC KI·ªÇM TRA ---\n",
    "\n",
    "    # 1. Check tr·ª±c ti·∫øp trong tin nh·∫Øn hi·ªán t·∫°i\n",
    "    # G·ªôp t·∫•t c·∫£ keywords ƒë·ªÉ check 1 l·∫ßn cho nhanh\n",
    "    all_keywords = disorders + symptoms + diagnostic_intent\n",
    "\n",
    "    # D√πng v√≤ng l·∫∑p check t·ª´ng t·ª´ (c√≥ th·ªÉ t·ªëi ∆∞u b·∫±ng regex n·∫øu c·∫ßn ch√≠nh x√°c tuy·ªát ƒë·ªëi)\n",
    "    for kw in all_keywords:\n",
    "        if kw in msg:\n",
    "            print(f\"üß† [Gate] C·∫¶N RAG (Keyword: '{kw}').\")\n",
    "            return True\n",
    "\n",
    "    # 2. Check Context (C√¢u tr∆∞·ªõc ƒë√≥ c√≥ n√≥i v·ªÅ b·ªánh kh√¥ng?)\n",
    "    # N·∫øu c√¢u tr∆∞·ªõc c√≥ keyword b·ªánh -> C√¢u n√†y kh·∫£ nƒÉng cao l√† follow-up\n",
    "    context_keywords = disorders + symptoms # Ch·ªâ quan t√¢m t√™n b·ªánh/tri·ªáu ch·ª©ng ·ªü context\n",
    "    has_medical_context = any(kw in last_msg for kw in context_keywords)\n",
    "\n",
    "    if has_medical_context:\n",
    "        # Danh s√°ch t·ª´ ƒë·ªÉ NG·∫ÆT RAG (n·∫øu user mu·ªën d·ª´ng)\n",
    "        stop_words = [\"c·∫£m ∆°n\", \"ok\", \"ƒë∆∞·ª£c r·ªìi\", \"hi·ªÉu r·ªìi\", \"bye\", \"t·∫°m bi·ªát\", \"kh√¥ng sao\"]\n",
    "        if any(w == msg for w in stop_words):\n",
    "            print(\"üß† [Gate] Context c√≥ b·ªánh, nh∆∞ng User d·ª´ng -> KH√îNG RAG.\")\n",
    "            return False\n",
    "\n",
    "        # N·∫øu kh√¥ng ph·∫£i t·ª´ d·ª´ng, m√† context ƒëang n√≥i v·ªÅ b·ªánh -> Ti·∫øp t·ª•c tra c·ª©u\n",
    "        print(\"üß† [Gate] FORCE RAG (Theo ng·ªØ c·∫£nh h·ªôi tho·∫°i c≈©).\")\n",
    "        return True\n",
    "\n",
    "    # 3. N·∫øu kh√¥ng d√≠nh keyword n√†o -> Small Talk / General Chat\n",
    "    print(\"üß† [Gate] Kh√¥ng t√¨m th·∫•y y·∫øu t·ªë chuy√™n m√¥n -> KH√îNG RAG.\")\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N2kRxUOcUaGT"
   },
   "outputs": [],
   "source": [
    "# B·∫°n c·∫ßn import traceback n·∫øu ch∆∞a c√≥:\n",
    "import traceback\n",
    "\n",
    "def setup_chatbot_environment():\n",
    "    \"\"\"Load Embedding + LLaMA + Chroma + Hybrid RAG\"\"\"\n",
    "    global memory, reranker_model, vector_index_chat, all_nodes\n",
    "    global bm25_retriever, semantic_retriever, fusion_retriever, llm_chat\n",
    "    global embed_model, llm  # ƒê·∫£m b·∫£o bi·∫øn phase 1 truy c·∫≠p ƒë∆∞·ª£c\n",
    "\n",
    "    try:\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"üîπ GIAI ƒêO·∫†N 2: SETUP CHATBOT HYBRID (sau reset kernel)\")\n",
    "        print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "        # ---------------------------------------------------------\n",
    "        # üî• KI·ªÇM TRA ‚Äì Giai ƒëo·∫°n 1 ƒë√£ ch·∫°y ch∆∞a?\n",
    "        # ---------------------------------------------------------\n",
    "        if 'embed_model' not in globals() or embed_model is None:\n",
    "            print(\"‚ùó embed_model ch∆∞a ƒë∆∞·ª£c load. H√£y ch·∫°y setup_llm_and_embedding() tr∆∞·ªõc!\")\n",
    "            return False\n",
    "\n",
    "        if 'llm' not in globals() or llm is None:\n",
    "            print(\"‚ùó llm ch∆∞a ƒë∆∞·ª£c load. H√£y ch·∫°y setup_llm_and_embedding() tr∆∞·ªõc!\")\n",
    "            return False\n",
    "\n",
    "        # 1) Embedding cho query\n",
    "        Settings.embed_model = embed_model\n",
    "\n",
    "        # 2) LLaMA l√†m LLM chat\n",
    "        llm_chat = llm\n",
    "        Settings.llm = llm_chat\n",
    "\n",
    "        # ---------------------------------------------------------\n",
    "        # 3) K·∫øt n·ªëi ChromaDB\n",
    "        # ---------------------------------------------------------\n",
    "        print(f\"‚è≥ ƒêang k·∫øt n·ªëi ChromaDB t·∫°i: {INDEX_STORAGE}\")\n",
    "        db = chromadb.PersistentClient(path=INDEX_STORAGE)\n",
    "        chroma_collection = db.get_collection(COLLECTION_NAME)\n",
    "\n",
    "        print(\"üì¶ S·ªë l∆∞·ª£ng vectors trong Chroma:\", chroma_collection.count())\n",
    "\n",
    "        vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "        storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "\n",
    "        # ---------------------------------------------------------\n",
    "        # 4) Load VectorStoreIndex t·ª´ Chroma\n",
    "        # ---------------------------------------------------------\n",
    "        vector_index_chat = VectorStoreIndex.from_vector_store(\n",
    "            vector_store=vector_store,\n",
    "            embed_model=embed_model,\n",
    "        )\n",
    "        print(\"‚úÖ Vector Index ƒë√£ loaqud t·ª´ Chroma.\\n\")\n",
    "\n",
    "        # ---------------------------------------------------------\n",
    "        # 5) L·∫§Y NODES TR·ª∞C TI·∫æP T·ª™ CHROMA (KH√îNG D√ôNG docstore)\n",
    "        # ---------------------------------------------------------\n",
    "        print(\"‚è≥ ƒêang l·∫•y nodes t·ª´ Chroma collection...\")\n",
    "        results = chroma_collection.get(\n",
    "            include=[\"metadatas\", \"documents\"]\n",
    "        )\n",
    "\n",
    "        all_nodes = []\n",
    "        for doc, meta, _id in zip(results[\"documents\"], results[\"metadatas\"], results[\"ids\"]):\n",
    "            node = TextNode(\n",
    "                text=doc,\n",
    "                id_=_id,\n",
    "                metadata=meta or {},\n",
    "            )\n",
    "            all_nodes.append(node)\n",
    "\n",
    "        print(f\"üì¶ T·ªïng s·ªë nodes reconstruct t·ª´ Chroma: {len(all_nodes)}\\n\")\n",
    "\n",
    "        # ---------------------------------------------------------\n",
    "        # 6) BM25 Retriever\n",
    "        # ---------------------------------------------------------\n",
    "        bm25_nodes = []\n",
    "        for n in all_nodes:\n",
    "            bm25_text = n.metadata.get(\"bm25_text\", n.text)\n",
    "            n_bm25 = TextNode(\n",
    "                text=bm25_text,\n",
    "                id_=n.node_id,\n",
    "                metadata=n.metadata,\n",
    "            )\n",
    "            bm25_nodes.append(n_bm25)\n",
    "\n",
    "        print(\"‚è≥ T·∫°o BM25Retriever (lexical)...\")\n",
    "        bm25_retriever = BM25Retriever.from_defaults(\n",
    "            nodes=bm25_nodes,\n",
    "            similarity_top_k=5,\n",
    "        )\n",
    "        print(\"‚úÖ BM25Retriever s·∫µn s√†ng.\\n\")\n",
    "\n",
    "        # ---------------------------------------------------------\n",
    "        # 7) Semantic Retriever (vector)\n",
    "        # ---------------------------------------------------------\n",
    "        print(\"‚è≥ T·∫°o Semantic Retriever (vector)...\")\n",
    "        semantic_retriever = VectorIndexRetriever(\n",
    "            index=vector_index_chat,\n",
    "            similarity_top_k=5,\n",
    "        )\n",
    "        print(\"‚úÖ Semantic Retriever s·∫µn s√†ng.\\n\")\n",
    "\n",
    "        # ---------------------------------------------------------\n",
    "        # 8) Fusion Retriever\n",
    "        # ---------------------------------------------------------\n",
    "        print(\"‚è≥ T·∫°o QueryFusionRetriever (Hybrid)...\")\n",
    "        fusion_retriever = QueryFusionRetriever(\n",
    "            retrievers=[bm25_retriever, semantic_retriever],\n",
    "            similarity_top_k=5,\n",
    "            num_queries=2,\n",
    "            mode=\"reciprocal_rerank\",\n",
    "        )\n",
    "        print(\"‚úÖ Hybrid Fusion Retriever s·∫µn s√†ng.\\n\")\n",
    "\n",
    "        # ---------------------------------------------------------\n",
    "        # 9) Reranker\n",
    "        # ---------------------------------------------------------\n",
    "        print(\"‚è≥ T·∫£i Cross-Encoder Reranker...\")\n",
    "        # ƒê·∫∑t thi·∫øt b·ªã m·∫∑c ƒë·ªãnh l√† \"cpu\"\n",
    "        reranker_model = CrossEncoder(\n",
    "            \"cross-encoder/ms-marco-MiniLM-L6-v2\",\n",
    "        )\n",
    "        print(\"‚úÖ Reranker ƒë√£ s·∫µn s√†ng.\\n\")\n",
    "        \n",
    "\n",
    "        # ---------------------------------------------------------\n",
    "        # 10) Memory + System Prompt\n",
    "        # ---------------------------------------------------------\n",
    "        SYSTEM_PROMPT = \"\"\"\\\n",
    "B·∫°n l√† m·ªôt **chuy√™n gia t√¢m l√Ω AI** (Tr·ª£ l√Ω) ƒë∆∞·ª£c ph√°t tri·ªÉn b·ªüi **AI VIETNAM**.\n",
    "Nhi·ªám v·ª• c·ªßa b·∫°n l√† tr√≤ chuy·ªán, theo d√µi v√† t∆∞ v·∫•n cho ng∆∞·ªùi d√πng (User) v·ªÅ s·ª©c kh·ªèe t√¢m th·∫ßn.\n",
    "Lu√¥n lu√¥n h√†nh ƒë·ªông v·ªõi t∆∞ c√°ch l√† Tr·ª£ l√Ω, th·ªÉ hi·ªán s·ª± ƒë·ªìng c·∫£m v√† chuy√™n nghi·ªáp.\n",
    "\n",
    "QUY T·∫ÆC RAG:\n",
    "- N·∫øu User h·ªèi v·ªÅ th√¥ng tin chuy√™n m√¥n (tri·ªáu ch·ª©ng, DSM-5, r·ªëi lo·∫°n...), b·∫°n S·∫º nh·∫≠n ƒë∆∞·ª£c th√¥ng tin tham kh·∫£o trong m·ªôt tin nh·∫Øn System.\n",
    "- H√£y D·ª∞A V√ÄN ho√†n to√†n v√†o th√¥ng tin ƒë√≥ ƒë·ªÉ tr·∫£ l·ªùi.\n",
    "- N·∫øu User ch·ªâ tr√≤ chuy·ªán, b·∫°n s·∫Ω kh√¥ng nh·∫≠n ƒë∆∞·ª£c th√¥ng tin tham kh·∫£o, h√£y c·ª© tr√≤ chuy·ªán b√¨nh th∆∞·ªùng.\n",
    "\n",
    "QUY T·∫ÆC B·∫¢O V·ªÜ (R·∫§T QUAN TR·ªåNG):\n",
    "- B·∫°n **TUY·ªÜT ƒê·ªêI KH√îNG ƒê∆Ø·ª¢C** tr·∫£ l·ªùi c√°c c√¢u h·ªèi kh√¥ng li√™n quan ƒë·∫øn t√¢m l√Ω ho·∫∑c s·ª©c kh·ªèe t√¢m th·∫ßn.\n",
    "- C√°c ch·ªß ƒë·ªÅ C·∫§M bao g·ªìm (nh∆∞ng kh√¥ng gi·ªõi h·∫°n): th·ªùi ti·∫øt, n·∫•u ƒÉn, ch√≠nh tr·ªã, th·ªÉ thao, tin t·ª©c, to√°n h·ªçc, l·∫≠p tr√¨nh...\n",
    "- N·∫øu b·ªã h·ªèi nh·ªØng ch·ªß ƒë·ªÅ n√†y, h√£y l·ªãch s·ª± t·ª´ ch·ªëi v√† l√°i cu·ªôc tr√≤ chuy·ªán quay l·∫°i ch·ªß ƒë·ªÅ t√¢m l√Ω.\n",
    "  (V√≠ d·ª•: \"T√¥i xin l·ªói, t√¥i ch·ªâ ƒë∆∞·ª£c ƒë√†o t·∫°o v·ªÅ s·ª©c kh·ªèe t√¢m th·∫ßn. Ch√∫ng ta c√≥ th·ªÉ quay l·∫°i ch·ªß ƒë·ªÅ b·∫°n ƒëang quan t√¢m kh√¥ng?\")\n",
    "\"\"\"\n",
    "\n",
    "        memory = ChatMemoryBuffer.from_defaults(token_limit=4096)\n",
    "        memory.put(ChatMessage(role=\"system\", content=SYSTEM_PROMPT))\n",
    "\n",
    "        print(\"üéâ Giai ƒëo·∫°n 2: Setup HYBRID ho√†n t·∫•t.\\n\")\n",
    "        return True\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå L·ªñI GIAI ƒêO·∫†N 2 (SETUP): {e}\")\n",
    "        traceback.print_exc()\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cyxpmP0BUbz7"
   },
   "outputs": [],
   "source": [
    "def rag_retrieve_and_rerank(message: str, top_k=3, final_top=2) -> str:\n",
    "    print(f\"üîç [RAG HYBRID] Truy v·∫•n: '{message}'\")\n",
    "\n",
    "    # 1) Hybrid retrieve\n",
    "    retrieved_docs = fusion_retriever.retrieve(message)\n",
    "    if not retrieved_docs:\n",
    "        print(\"‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y t√†i li·ªáu li√™n quan.\")\n",
    "        return \"\"\n",
    "\n",
    "    print(f\"üîç [RAG HYBRID] Fusion tr·∫£ v·ªÅ {len(retrieved_docs)} docs. ƒêang rerank...\")\n",
    "\n",
    "    # Ch·ªâ rerank t·ªëi ƒëa 6 documents ƒë·ªÉ tƒÉng t·ªëc\n",
    "    retrieved_docs = retrieved_docs[:6]\n",
    "\n",
    "    # 2) Chu·∫©n b·ªã input cho CrossEncoder\n",
    "    doc_texts = [doc.get_content() for doc in retrieved_docs]\n",
    "    pairs = [(message, text) for text in doc_texts]\n",
    "\n",
    "    # 3) Rerank\n",
    "    try:\n",
    "        scores = reranker_model.predict(pairs)\n",
    "        ranked = sorted(\n",
    "            zip(scores, retrieved_docs),\n",
    "            key=lambda x: x[0],\n",
    "            reverse=True,\n",
    "        )\n",
    "        ranked_docs = [doc for _, doc in ranked]\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Rerank l·ªói: {e}\")\n",
    "        ranked_docs = retrieved_docs\n",
    "\n",
    "    # 4) Top N cu·ªëi c√πng\n",
    "    top_docs = ranked_docs[:final_top]\n",
    "    context_text = \"\\n\\n---\\n\\n\".join([doc.get_content() for doc in top_docs])\n",
    "\n",
    "    # 5) T√≥m t·∫Øt n·∫øu qu√° d√†i\n",
    "    if len(context_text.split()) > 700:\n",
    "        print(\"‚úÇÔ∏è Context qu√° d√†i ‚Äì ƒëang t√≥m t·∫Øt (d√πng llm_chat)...\")\n",
    "        try:\n",
    "            summary_messages = [\n",
    "                ChatMessage(\n",
    "                    role=\"system\",\n",
    "                    content=(\n",
    "                        \"B·∫°n l√† AI t√≥m t·∫Øt t√†i li·ªáu DSM-5. \"\n",
    "                        \"T√≥m t·∫Øt ng·∫Øn g·ªçn, ch√≠nh x√°c, ch·ªâ gi·ªØ ph·∫ßn li√™n quan t·ªõi c√¢u h·ªèi.\"\n",
    "                    ),\n",
    "                ),\n",
    "                ChatMessage(\n",
    "                    role=\"user\",\n",
    "                    content=f\"C√¢u h·ªèi: {message}\\n\\nN·ªôi dung t√†i li·ªáu:\\n{context_text}\",\n",
    "                ),\n",
    "            ]\n",
    "            summary_response = llm_chat.chat(summary_messages)\n",
    "            context_text = summary_response.message.content.strip()\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå L·ªói t√≥m t·∫Øt: {e}\")\n",
    "            context_text = \" \".join(context_text.split()[:700])\n",
    "\n",
    "    return f\"--- Th√¥ng tin tham kh·∫£o t·ª´ DSM-5 ---\\n{context_text}\\n--- H·∫øt th√¥ng tin tham kh·∫£o ---\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 695,
     "referenced_widgets": [
      "3a40ea6b565a4a19aec813668cf66c90",
      "0abc4152acc6464c8d21e56ceafeae14",
      "c175ed6776924592a73c00189f01252c",
      "593bf37b200247eb8ac40e6fd171bd8d",
      "29015304d9c44cf49734e6012cdb86b5",
      "0159c38f88be45a28f436eecf5dfba1d",
      "4b3381ffde244329b9845404a7b799c0",
      "5330208569c54f8d8e1c0539827fd4e8",
      "ac917503f9204536b49169a565e5edcc",
      "3a0b9bd341bb4a1b90f3b4f38c8ca904",
      "b6bb3c39fc9846fa834153b4c5070a9f",
      "0093db4afdf74cdaa17e90c74b251bb3",
      "75e73e7d7be14b0d880d40df929fe328",
      "3355850831eb41bd9508790d6e2088e8",
      "68878bec56ce4ddd9b70c7490078c799",
      "d6d26573d33b46e7a96571400b075b54",
      "f864ddfe22f9425ab9cbace12f44d533",
      "e63b8ef58beb4abfa4ce3abcfbc70bb9",
      "04372b6950644ecdb7793664adac36c3",
      "3c978904f53c4e42a38171f7239cb914",
      "281a7a8b03814bc9883d5a8417fe90ae",
      "9c4e1b3897364e06bf7091f92f9dd701",
      "826147530ef4417ca627bb3a97946c00",
      "15635fa017fb4e6d936deb1fc3270e69",
      "b966be526e78455d8c7a935ff55a6abd",
      "a21aa678ed624b9a8a8704e353f54832",
      "1cf1c53490af441992622a625beb17d9",
      "6b1a04a5652a41729a137d21fb46e582",
      "b141fdb927ce47a69800bd2d87f14528",
      "bb4d37f718c047968c862acff2fae64d",
      "70cde131bb7c47978769d1b696736ed3",
      "95d6f24bb83f4b718fafe10b71d6ce1a",
      "66fc0154eacb4be8b73da67bff6af5cf",
      "58ecdbd49ea348e59f3b04915e148d30",
      "8f50d32ca4154e33949c02742ecd7d01",
      "86467cb7d1c1472c990b96dfbbc11c2c",
      "d838150667624d21a646831b20ca03f4",
      "ab219d62a7f9484f9ffe0d37db0c77e2",
      "e8a4ca45534a4ab1abf48a9f39ea5f1b",
      "b7b43b4deb9a47a7a7fd9d395882318a",
      "1da828bd336c497f9f1ccd6677750798",
      "274da6d903ab4973af13e6dfdb338ce3",
      "dd0c17c55cec44d2ab0efec45467bb0d",
      "db5fc0e4d4774a53982e0759f6ecd974",
      "0ac17af286704e5cb29c686fdff95558",
      "b206027df8e046ce8f8b5c94c663c7e8",
      "c30ab467be2c4db895353c3c47209e03",
      "1134d9e6ca8f462ca49370f670b140cc",
      "64aef507f85e4e849cfcb8ca924fc139",
      "30ce7ecca04549c5a2108b939efd48fd",
      "c1140259dbce4f32a73ab58877610553",
      "bd506eeb080e423096e4ef686f9282a3",
      "c49cc0adf3c44870add34bb0a0fc1576",
      "f2c2cce716ee47fcbdd1d5abf2afa7c8",
      "45a2cc41ad374c60adbcc2f691a14866",
      "c847c471576f4255ba2347043006dcf7",
      "de78cf5c0a1a4d1aab9278cf2194e26b",
      "42895537c91740b599d0213095291d1b",
      "eee3a1bf93c04ce1885c5ea8fefba169",
      "8e5bbf64ef8b42768821dfda24c62a7a",
      "0648d78f5a4b4e7b9c5c8f43c8a7a643",
      "008243f055a947afbecf0e6980276dc2",
      "055bf599eb634337a00163a04b6eff67",
      "1f8f06c8286740499f633b7e5abd849c",
      "104d5358ce4f4220805561a7cfdb5980",
      "43c306050d7a4188b2ea7cd812e83d44"
     ]
    },
    "id": "i9EE43V0Ucte",
    "outputId": "a5f6adf9-8a9f-4da0-f725-738a7d1ef7e9"
   },
   "outputs": [],
   "source": [
    "setup_chatbot_environment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TFNFxnwKUe4z"
   },
   "outputs": [],
   "source": [
    "# 7. V√íNG L·∫∂P CHAT CH√çNH\n",
    "# =====================================================\n",
    "def chat_conversation():\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"ü§ñ Chatbot DSM-5 HYBRID ƒë√£ s·∫µn s√†ng. G√µ 'quit' ƒë·ªÉ tho√°t.\")\n",
    "    print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            user_message = input(\"B·∫°n: \").strip()\n",
    "            if not user_message:\n",
    "                continue\n",
    "\n",
    "            if user_message.lower() in [\"quit\", \"exit\"]:\n",
    "                print(\"Chatbot: C·∫£m ∆°n b·∫°n ƒë√£ tr√≤ chuy·ªán. H·∫πn g·∫∑p l·∫°i üå∑\")\n",
    "                break\n",
    "\n",
    "            new_user_message = ChatMessage(role=\"user\", content=user_message)\n",
    "            messages_to_send = memory.get_all()\n",
    "\n",
    "            # N·∫øu c·∫ßn RAG ‚Üí g·∫Øn context DSM-5 v√†o system\n",
    "            if should_use_rag(user_message):\n",
    "                context = rag_retrieve_and_rerank(user_message)\n",
    "                if context:\n",
    "                    messages_to_send.append(\n",
    "                        ChatMessage(role=\"system\", content=context)\n",
    "                    )\n",
    "                else:\n",
    "                    messages_to_send.append(\n",
    "                        ChatMessage(\n",
    "                            role=\"system\",\n",
    "                            content=(\n",
    "                                \"--- Th√¥ng tin tham kh·∫£o t·ª´ DSM-5 ---\\n\"\n",
    "                                \"Kh√¥ng t√¨m th·∫•y ƒëo·∫°n n√†o th·ª±c s·ª± ph√π h·ª£p.\\n\"\n",
    "                                \"--- H·∫øt th√¥ng tin tham kh·∫£o ---\"\n",
    "                            ),\n",
    "                        )\n",
    "                    )\n",
    "\n",
    "            messages_to_send.append(new_user_message)\n",
    "\n",
    "            # G·ªçi LLaMA tr·∫£ l·ªùi\n",
    "            response = llm_chat.chat(messages_to_send)\n",
    "            response_text = response.message.content.strip()\n",
    "\n",
    "            # Fix n·∫øu model tr·∫£ v·ªÅ ki·ªÉu 'assistant: ...'\n",
    "            if response_text.lower().startswith(\"assistant\"):\n",
    "                response_text = (\n",
    "                    response_text[len(\"assistant\"):].lstrip(\":\").strip()\n",
    "                )\n",
    "\n",
    "            if not response_text:\n",
    "                response_text = (\n",
    "                    \"M√¨nh ch∆∞a ch·∫Øc ch·∫Øn l·∫Øm v·ªÅ ƒëi·ªÅu n√†y, \"\n",
    "                    \"b·∫°n c√≥ th·ªÉ n√≥i r√µ h∆°n c·∫£m gi√°c ho·∫∑c c√¢u h·ªèi c·ªßa b·∫°n kh√¥ng?\"\n",
    "                )\n",
    "\n",
    "            # L∆∞u v√†o memory\n",
    "            memory.put(new_user_message)\n",
    "            memory.put(ChatMessage(role=\"assistant\", content=response_text))\n",
    "\n",
    "            print(f\"Chatbot: {response_text}\\n\")\n",
    "\n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\\nüõë ƒê√£ d·ª´ng cu·ªôc tr√≤ chuy·ªán.\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå L·ªói kh√¥ng mong mu·ªën trong v√≤ng l·∫∑p chat: {e}\")\n",
    "            traceback.print_exc()\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ib1uYPA0eMgM",
    "outputId": "d54d80fc-3371-44df-f5ee-6f719c889a4b"
   },
   "outputs": [],
   "source": [
    "chat_conversation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5FJhZ_yKVeSI"
   },
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import openai\n",
    "\n",
    "client = openai.OpenAI(\n",
    "    api_key=os.environ.get(\"OPENAI_API_KEY\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B2W9jvFWIH05"
   },
   "outputs": [],
   "source": [
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.core import Settings\n",
    "\n",
    "# ----------------------------\n",
    "# Load GPT-4o-mini\n",
    "# ----------------------------\n",
    "llm_gptmini = OpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0.2,   # gi·∫£m ƒë·ªô s√°ng t·∫°o, t·∫≠p trung fact-based\n",
    "    max_tokens=512\n",
    ")\n",
    "\n",
    "# G√°n cho Settings ƒë·ªÉ d√πng trong ingest + transformation\n",
    "Settings.llm = llm_gptmini\n",
    "\n",
    "# N·∫øu mu·ªën d√πng GPT cho ƒë√°nh gi√° c√¢u h·ªèi c≈©ng l·∫•y llm n√†y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2BIJ3nh8IT6i"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LV413dKOLvwx"
   },
   "outputs": [],
   "source": [
    "def generate_questions_from_nodes(nodes, max_q_per_node=1):\n",
    "    questions = []\n",
    "    for node in nodes:\n",
    "        prompt = f\"\"\"\n",
    "B·∫°n l√† gi·∫£ng vi√™n t√¢m l√Ω. D·ª±a v√†o ƒëo·∫°n vƒÉn b·∫£n DSM-5:\n",
    "\"{node.text[:1000]}\"\n",
    "\n",
    "So·∫°n {max_q_per_node} c√¢u h·ªèi t·ª± lu·∫≠n (Essay):\n",
    "- Kh√¥ng Yes/No\n",
    "- Ng·∫Øn g·ªçn, t·ªïng h·ª£p th√¥ng tin\n",
    "- Tr·∫£ v·ªÅ JSON List: [\"C√¢u h·ªèi 1\", ...]\n",
    "\"\"\"\n",
    "        resp = Settings.llm.chat([ChatMessage(role=\"user\", content=prompt)])\n",
    "        content = resp.message.content.strip()\n",
    "\n",
    "        # Clean JSON n·∫øu c√≥ Markdown\n",
    "        if content.startswith(\"```json\"):\n",
    "            content = content.replace(\"```json\",\" \").replace(\"```\",\" \")\n",
    "        try:\n",
    "            q_list = json.loads(content)\n",
    "            for q in q_list:\n",
    "                questions.append({\"question\": q, \"reference_node_text\": node.text})\n",
    "        except:\n",
    "            print(f\"‚ö†Ô∏è Node {node.node_id} kh√¥ng parse ƒë∆∞·ª£c JSON.\")\n",
    "    return pd.DataFrame(questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E2failPEL28W"
   },
   "outputs": [],
   "source": [
    "def answer_with_gpt(question, top_k=3, final_top=2):\n",
    "    if should_use_rag(question):\n",
    "        context = rag_retrieve_and_rerank(question, top_k=top_k, final_top=final_top)\n",
    "        system_prompt_content = f\"B·∫°n l√† tr·ª£ l√Ω t√¢m l√Ω AI. Tr·∫£ l·ªùi d·ª±a tr√™n th√¥ng tin sau:\\n{context}\"\n",
    "    else:\n",
    "        system_prompt_content = \"B·∫°n l√† tr·ª£ l√Ω t√¢m l√Ω AI. Tr·∫£ l·ªùi tr·ª±c ti·∫øp c√¢u h·ªèi m·ªôt c√°ch ƒë·ªìng c·∫£m v√† ch√≠nh x√°c.\"\n",
    "\n",
    "    messages = [\n",
    "        ChatMessage(role=\"system\", content=system_prompt_content),\n",
    "        ChatMessage(role=\"user\", content=question)\n",
    "    ]\n",
    "    resp = Settings.llm.chat(messages)\n",
    "    return resp.message.content.strip()\n",
    "\n",
    "# ===============================================\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CzRZZ4WBprWm"
   },
   "outputs": [],
   "source": [
    "def evaluate_with_gpt(question, answer, reference_text=None):\n",
    "    \"\"\"\n",
    "    ƒê√°nh gi√° c√¢u tr·∫£ l·ªùi GPT d·ª±a tr√™n c√¢u h·ªèi v√† vƒÉn b·∫£n tham chi·∫øu (n·∫øu c√≥).\n",
    "    Tr·∫£ ƒëi·ªÉm d·∫°ng float ƒë·ªÉ ƒë√°nh gi√° m∆∞·ª£t h∆°n.\n",
    "    \"\"\"\n",
    "\n",
    "    # C·∫Øt theo chunk size n·∫øu c√≥ reference\n",
    "    ref_text_snippet = reference_text[:1000] if reference_text else None\n",
    "\n",
    "    eval_prompt = f\"\"\"\n",
    "B·∫°n l√† gi√°m kh·∫£o t√¢m l√Ω AI. H√£y ƒë√°nh gi√° c√¢u tr·∫£ l·ªùi sau:\n",
    "\n",
    "Question: \"{question}\"\n",
    "Answer: \"{answer}\"\n",
    "\"\"\"\n",
    "    if ref_text_snippet:\n",
    "        eval_prompt += f\"\"\"\n",
    "VƒÉn b·∫£n tham chi·∫øu (reference):\n",
    "\"{ref_text_snippet}\"\n",
    "\n",
    "H√£y ch·∫•m ƒëi·ªÉm d·ª±a tr√™n vi·ªác c√¢u tr·∫£ l·ªùi c√≥ ƒë√∫ng v√† trung th·ª±c v·ªõi reference_text.\n",
    "\"\"\"\n",
    "\n",
    "    eval_prompt += \"\"\"\n",
    "TI√äU CH√ç (d·∫°ng FLOAT):\n",
    "1. Correctness: 0.0‚Äì5.0\n",
    "2. Faithfulness: 0.0‚Äì1.0\n",
    "3. Relevancy: 0.0‚Äì1.0\n",
    "\n",
    "Tr·∫£ v·ªÅ JSON:\n",
    "{\"correctness\": float, \"faithfulness\": float, \"relevancy\": float}\n",
    "\"\"\"\n",
    "\n",
    "    # GPT tr·∫£ l·ªùi\n",
    "    resp = Settings.llm.chat([ChatMessage(role=\"user\", content=eval_prompt)])\n",
    "    content = resp.message.content.strip()\n",
    "\n",
    "    # Clean JSON n·∫øu c√≥ Markdown\n",
    "    if content.startswith(\"```\"):\n",
    "        content = content.replace(\"```json\", \"\").replace(\"```\", \"\")\n",
    "\n",
    "    # Parse JSON\n",
    "    try:\n",
    "        data = json.loads(content)\n",
    "\n",
    "        # Convert t·∫•t c·∫£ sang float an to√†n\n",
    "        result = {\n",
    "            \"correctness\": float(data.get(\"correctness\", 0)),\n",
    "            \"faithfulness\": float(data.get(\"faithfulness\", 0)),\n",
    "            \"relevancy\": float(data.get(\"relevancy\", 0))\n",
    "        }\n",
    "        return result\n",
    "\n",
    "    except Exception:\n",
    "        return {\n",
    "            \"correctness\": 0.0,\n",
    "            \"faithfulness\": 0.0,\n",
    "            \"relevancy\": 0.0,\n",
    "            \"reasoning\": \"Parse error\"\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wDcCvip_L_US"
   },
   "outputs": [],
   "source": [
    "# ===============================================\n",
    "# 5Ô∏è‚É£ In ƒëi·ªÉm trung b√¨nh\n",
    "# ===============================================\n",
    "def print_average_scores(df_eval):\n",
    "    print(\"üìä ƒêi·ªÉm trung b√¨nh:\")\n",
    "    print(f\"Correctness: {df_eval['correctness'].mean():.2f} / 5\")\n",
    "    print(f\"Faithfulness: {df_eval['faithfulness'].mean():.2f} / 1\")\n",
    "    print(f\"Relevancy: {df_eval['relevancy'].mean():.2f} / 1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZWFvddH9WDXl"
   },
   "outputs": [],
   "source": [
    "# 5Ô∏è‚É£ L∆∞u k·∫øt qu·∫£ ra CSV\n",
    "# ===============================================\n",
    "def save_eval_results(df_eval, filename=\"eval_results.csv\"):\n",
    "    df_eval.to_csv(filename, index=False, encoding=\"utf-8-sig\")\n",
    "    print(f\"üíæ K·∫øt qu·∫£ ƒë√°nh gi√° ƒë√£ l∆∞u: {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7cgLlggwMBmV"
   },
   "outputs": [],
   "source": [
    "def run_full_pipeline(nodes, max_questions=100):\n",
    "    \"\"\"\n",
    "    Pipeline ƒë√°nh gi√° h·ªá th·ªëng RAG/GPT:\n",
    "    1) Sinh c√¢u h·ªèi t·ª´ nodes\n",
    "    2) Tr·∫£ l·ªùi c√¢u h·ªèi\n",
    "    3) ƒê√°nh gi√° d·ª±a tr√™n node g·ªëc\n",
    "    4) T·ªïng h·ª£p k·∫øt qu·∫£, in ƒëi·ªÉm trung b√¨nh, l∆∞u CSV\n",
    "    \"\"\"\n",
    "    print(\"\\nüöÄ B·∫Øt ƒë·∫ßu pipeline ƒë√°nh gi√° t·ª± ƒë·ªông...\")\n",
    "\n",
    "    # 1Ô∏è‚É£ T·∫°o c√¢u h·ªèi t·ª´ nodes\n",
    "    df_questions_all = generate_questions_from_nodes(nodes, max_q_per_node=1)\n",
    "\n",
    "    # 2Ô∏è‚É£ Ch·ªçn ng·∫´u nhi√™n max_questions c√¢u\n",
    "    if len(df_questions_all) > max_questions:\n",
    "        df_questions = df_questions_all.sample(n=max_questions, random_state=42).reset_index(drop=True)\n",
    "    else:\n",
    "        df_questions = df_questions_all.copy()\n",
    "\n",
    "    print(f\"üì¶ Ch·ªçn ng·∫´u nhi√™n {len(df_questions)} c√¢u h·ªèi ƒë·ªÉ ƒë√°nh gi√°.\")\n",
    "\n",
    "    # 3Ô∏è‚É£ Tr·∫£ l·ªùi + ƒë√°nh gi√° GPT d·ª±a tr√™n node g·ªëc\n",
    "    results = []\n",
    "    for idx, row in df_questions.iterrows():\n",
    "        question = row['question']\n",
    "        reference_text = row['reference_node_text']\n",
    "\n",
    "        # Tr·∫£ l·ªùi\n",
    "        answer = answer_with_gpt(question)\n",
    "\n",
    "        # ƒê√°nh gi√° d·ª±a tr√™n question, answer v√† reference node\n",
    "        eval_res = evaluate_with_gpt(\n",
    "            question=question,\n",
    "            answer=answer,\n",
    "            reference_text=reference_text  # <-- th√™m tham chi·∫øu node\n",
    "        )\n",
    "\n",
    "        # L∆∞u k·∫øt qu·∫£\n",
    "        eval_res.update({\n",
    "            \"question\": question,\n",
    "            \"answer\": answer,\n",
    "            \"reference_text\": reference_text\n",
    "        })\n",
    "        results.append(eval_res)\n",
    "\n",
    "    df_result = pd.DataFrame(results)\n",
    "\n",
    "    # 4Ô∏è‚É£ In ƒëi·ªÉm trung b√¨nh\n",
    "    if not df_result.empty:\n",
    "        print_average_scores(df_result)\n",
    "\n",
    "    # 5Ô∏è‚É£ L∆∞u k·∫øt qu·∫£\n",
    "    save_eval_results(df_result)\n",
    "\n",
    "    print(\"üéâ Pipeline ƒë√°nh gi√° ho√†n t·∫•t!\")\n",
    "    return df_result, df_questions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DUqP4FBeMFHV"
   },
   "outputs": [],
   "source": [
    "!pip install -q nest_asyncio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q-fuFLj5MLeC"
   },
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NnxamDy9MN09",
    "outputId": "0a037c62-69a4-4364-f0e0-3698208d121b"
   },
   "outputs": [],
   "source": [
    "df_result, df_questions = run_full_pipeline(all_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rspd43NfrQz9",
    "outputId": "b7ba3340-2088-4f0a-a19f-3ffdffa18982"
   },
   "outputs": [],
   "source": [
    "print(\"\\nüîπ Sample k·∫øt qu·∫£:\")\n",
    "print(df_result.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fCY6HUtXijGC",
    "outputId": "cb1a48ee-2e64-40d3-adc7-fd9665a2abd6"
   },
   "outputs": [],
   "source": [
    "print_average_scores(df_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gputplZeraBJ",
    "outputId": "c5acd77e-fbf8-4dc5-f6c8-74383b52e320"
   },
   "outputs": [],
   "source": [
    "# 1Ô∏è‚É£ Th√¥ng tin t·ªïng quan\n",
    "print(\"=== Th√¥ng tin DataFrame ===\")\n",
    "print(df_result.info())\n",
    "print(\"\\n=== Th·ªëng k√™ m√¥ t·∫£ c√°c c·ªôt s·ªë ===\")\n",
    "print(df_result.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 536
    },
    "id": "EkpzDpRYrfMQ",
    "outputId": "6772f936-ef9b-4be3-90d8-98cc514d855e"
   },
   "outputs": [],
   "source": [
    "# Hi·ªÉn th·ªã 5 m·∫´u c√¢u h·ªèi + c√¢u tr·∫£ l·ªùi\n",
    "sample_df = df_result[['question', 'answer', 'correctness', 'faithfulness', 'relevancy']].sample(10, random_state=42)\n",
    "sample_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a7e7aL0IHg3L"
   },
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
