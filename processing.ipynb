{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jMytw5mGUV6L",
    "outputId": "7788c892-7d7f-4c2c-da0e-afc4565830cd"
   },
   "outputs": [],
   "source": [
    "!pip install -q jedi\n",
    "\n",
    "!pip install -q --upgrade \\\n",
    "    llama-index \\\n",
    "    llama-index-core \\\n",
    "    transformers \\\n",
    "    accelerate \\\n",
    "    bitsandbytes\n",
    "\n",
    "!pip install -q \\\n",
    "    sentence-transformers \\\n",
    "    chromadb \\\n",
    "    llama-index-vector-stores-chroma \\\n",
    "    llama-index-llms-huggingface \\\n",
    "    llama-index-embeddings-huggingface \\\n",
    "    llama-index-retrievers-bm25\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Mkx2YPjsycN2",
    "outputId": "41166e93-092b-4b24-c79e-76a8a3d51010"
   },
   "outputs": [],
   "source": [
    "import pkg_resources\n",
    "\n",
    "packages = [\n",
    "    \"jedi\",\n",
    "    \"llama-index\",\n",
    "    \"llama-index-core\",\n",
    "    \"transformers\",\n",
    "    \"accelerate\",\n",
    "    \"bitsandbytes\",\n",
    "    \"sentence-transformers\",\n",
    "    \"chromadb\",\n",
    "    \"llama-index-vector-stores-chroma\",\n",
    "    \"llama-index-llms-huggingface\",\n",
    "    \"llama-index-embeddings-huggingface\",\n",
    "    \"llama-index-retrievers-bm25\"\n",
    "]\n",
    "\n",
    "for pkg in packages:\n",
    "    try:\n",
    "        version = pkg_resources.get_distribution(pkg).version\n",
    "        print(f\"{pkg} == {version}\")\n",
    "    except Exception as e:\n",
    "        print(f\"{pkg} ‚ùå NOT INSTALLED\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RMgQSWm7ygrm",
    "outputId": "4683982d-7346-4d52-932f-bfd70731b8d1"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import platform\n",
    "import torch\n",
    "\n",
    "# Python version\n",
    "print(\"üêç Python version:\", sys.version)\n",
    "\n",
    "# OS + m√¥i tr∆∞·ªùng\n",
    "print(\"üíª OS:\", platform.platform())\n",
    "print(\"üè∑Ô∏è  Python executable:\", sys.executable)\n",
    "\n",
    "# Th·ª≠ nh·∫≠n d·∫°ng m√¥i tr∆∞·ªùng notebook\n",
    "try:\n",
    "    import google.colab\n",
    "    print(\"üåê Environment: Google Colab\")\n",
    "except:\n",
    "    from IPython import get_ipython\n",
    "    shell = get_ipython().__class__.__name__\n",
    "    if shell == 'ZMQInteractiveShell':\n",
    "        print(\"üß™ Environment: Jupyter / VSCode Notebook\")\n",
    "    elif shell == 'TerminalInteractiveShell':\n",
    "        print(\"üìü Environment: Terminal / Local Python\")\n",
    "    else:\n",
    "        print(\"‚ùì Environment: Unknown\")\n",
    "\n",
    "# Torch info\n",
    "try:\n",
    "    print(\"üî• Torch version:\", torch.__version__)\n",
    "    print(\"‚ö° CUDA available:\", torch.cuda.is_available())\n",
    "    if torch.cuda.is_available():\n",
    "        print(\"üîã GPU name:\", torch.cuda.get_device_name(0))\n",
    "except:\n",
    "    print(\"‚ùå Torch is not installed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "62uMd__cUDyN",
    "outputId": "72d29846-3dc8-4572-e150-07b39d00030e"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rA7DDHGIUFJs",
    "outputId": "a80befeb-97b9-4483-bbed-5fc92f5517d0"
   },
   "outputs": [],
   "source": [
    "!hf auth login\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iZ-CtySXUGeV"
   },
   "outputs": [],
   "source": [
    "DOCX_PATH = \"/content/drive/MyDrive/DTCNTT/DMS-5.pdf\"\n",
    "CACHE_FILE = \"/content/drive/MyDrive/DTCNTT/data/cache/pipeline_cache.json\"\n",
    "INDEX_STORAGE = \"/content/drive/MyDrive/DTCNTT/data/index_store_V6_maximalist\"\n",
    "COLLECTION_NAME = \"dsm5_V6_store\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wlsakg_yyIsq",
    "outputId": "249f76be-c32a-4ca4-8ea8-d1e8ebe32a95"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T0rthKGuxuwz"
   },
   "outputs": [],
   "source": [
    "# --- C√°c th∆∞ vi·ªán h·ªá th·ªëng & ti·ªán √≠ch ---\n",
    "import traceback\n",
    "import gc\n",
    "import torch\n",
    "import chromadb\n",
    "torch.cuda.empty_cache()\n",
    "# --- Hugging Face & Sentence Transformers ---\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from sentence_transformers import CrossEncoder\n",
    "\n",
    "# --- LlamaIndex Core & LLM ---\n",
    "from llama_index.core import Settings, VectorStoreIndex, StorageContext\n",
    "from llama_index.core.schema import TextNode\n",
    "from llama_index.core.llms import ChatMessage\n",
    "from llama_index.core.memory import ChatMemoryBuffer\n",
    "\n",
    "# --- LlamaIndex Retrievers & Storage ---\n",
    "from llama_index.llms.huggingface import HuggingFaceLLM\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "from llama_index.core.retrievers import VectorIndexRetriever, QueryFusionRetriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GUwrd8Bo8s0Q"
   },
   "outputs": [],
   "source": [
    "\n",
    "def setup_llm_and_embedding():\n",
    "    global embed_model, llm, model\n",
    "\n",
    "    # D·ªçn VRAM\n",
    "    print(\"üßπ Clearing VRAM...\")\n",
    "    for obj in [\"llm\", \"model\", \"embed_model\"]:\n",
    "        if obj in globals():\n",
    "            del globals()[obj]\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"‚úÖ VRAM cleared.\\n\")\n",
    "\n",
    "    # ------------------------------\n",
    "    # 1. Load Embedding\n",
    "    # ------------------------------\n",
    "    print(\"‚è≥ Loading embedding...\")\n",
    "    embed_model = HuggingFaceEmbedding(\n",
    "        model_name=\"AITeamVN/Vietnamese_Embedding\",\n",
    "        device=\"cpu\"\n",
    "    )\n",
    "    Settings.embed_model = embed_model\n",
    "    print(\"‚úÖ Embedding ready.\\n\")\n",
    "\n",
    "    # ------------------------------\n",
    "    # 2. Load Tokenizer t·ª´ HF HUB\n",
    "    # ------------------------------\n",
    "    REPO_ID = \"letri345/llama3-8b-merge\"  # <‚Äî s·ª≠a t√™n repo ·ªü ƒë√¢y\n",
    "\n",
    "    print(f\"‚è≥ Loading tokenizer from {REPO_ID}...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(REPO_ID, trust_remote_code=True)\n",
    "\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    # Gi·ªØ template chat ƒë√£ fine-tune\n",
    "    tokenizer.chat_template = (\n",
    "        \"<|begin_of_text|>\"\n",
    "        \"{% for message in messages %}\"\n",
    "        \"{% if message['role'] == 'user' %}\"\n",
    "            \"<|start_header_id|>user<|end_header_id|>\\n\\n{{ message['content'] }}<|eot_id|>\"\n",
    "        \"{% elif message['role'] == 'system' %}\"\n",
    "            \"<|start_header_id|>system<|end_header_id|>\\n\\n{{ message['content'] }}<|eot_id|>\"\n",
    "        \"{% elif message['role'] == 'assistant' %}\"\n",
    "            \"<|start_header_id|>assistant<|end_header_id|>\\n\\n{{ message['content'] }}<|eot_id|>\"\n",
    "        \"{% endif %}\"\n",
    "        \"{% endfor %}\"\n",
    "    )\n",
    "\n",
    "    # ------------------------------\n",
    "    # 3. Load Model 4-bit t·ª´ HF HUB\n",
    "    # ------------------------------\n",
    "    print(\"‚ö° Loading 4-bit model from HuggingFace Hub...\")\n",
    "\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "    )\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        REPO_ID,\n",
    "        device_map=\"auto\",\n",
    "        quantization_config=bnb_config,\n",
    "        trust_remote_code=True,\n",
    "    )\n",
    "\n",
    "    print(\"‚úÖ Model loaded successfully (4-bit).\")\n",
    "\n",
    "    # ------------------------------\n",
    "    # 4. Wrap cho LlamaIndex\n",
    "    # ------------------------------\n",
    "    eos_id = tokenizer.eos_token_id\n",
    "    eot_id = tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "\n",
    "    llm = HuggingFaceLLM(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        context_window=8192,\n",
    "        max_new_tokens=512,\n",
    "        generate_kwargs={\"temperature\": 0.3},\n",
    "        stopping_ids=[eos_id, eot_id],\n",
    "    )\n",
    "\n",
    "    Settings.llm = llm\n",
    "    print(\"\\nüéâ Llama 3 8B ƒë√£ s·∫µn s√†ng s·ª≠ d·ª•ng t·ª´ HuggingFace Hub!\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "76a60ce3b39847409e8602fa77aaa852",
      "f5904ea79609424abc7e58fa524e9ae6",
      "d4ccebf088034887b66b4d3ccaf9d20a",
      "60aa40b4d1aa433a8c5a852344a15243",
      "5e34afc1ef1d4ef9b9732a631269457d",
      "9104939ba6144605a08b0513972c28cd",
      "ef763a0cb61e4885b54a69eff5f4f2bc",
      "21152bb6faf443e3aceeffbf8b1e6446",
      "8da434b9df7045d2a15dd029e301e2a6",
      "56a6ae32e80841babf7f5a07b75563f1",
      "02bad3be82594ede96fa3cc87f46049b",
      "7f7e36622edd4b35ba1b4ff89dc2b123",
      "f7b55b3454434da08ce7746705d20fa2",
      "028d232651f048659cb0dca58e63c234",
      "1b95affa9bbf4b5b9355c01a95a4a860",
      "7915ee46de8f4aeda0d392c4e067ce08",
      "2d260c0a6b80450caef15d9471fe8356",
      "5a17e865720f4f27bdbd26a7e02d71d4",
      "bd8712ad1cca44b6a197e49525ed261d",
      "ace452169ebe48c19feb6cfb6715d848",
      "356c08e5cdeb4ef4905be31a3a7c273b",
      "079388d204c64695a9e99e89906ff106",
      "f4933874e6404ad39c4c1b7749c598db",
      "935c82a62c8b48e5b50707123e96cdf2",
      "e2678fff2550476e802e8e9eaf0b894f",
      "ce82c03cd2dc46bfa094ac1619d1124d",
      "05e6c7451d524d82b86c076c22203626",
      "3316cc4c1219401cb9f5184e4328ce8b",
      "96012c3054454a22a468d40cbf541781",
      "d7e4b0ef44104236a314de4b9a8c045c",
      "b264c41a053b4b2e8625785f04efc91b",
      "f384335adcd2428daf15454a6c5153c5",
      "9e95f0895193466ab20aaabfb0c96fda",
      "8633f9cb26414b8692e5cf6b03bbd6dc",
      "a082508c8c664cf2bb6b8f7ae3f59c3a",
      "e760632430da4c5cbb049e9a49d78f72",
      "bd8a9d72352d4006af4e6f8a5209afa9",
      "72268774d80448eeb876b5b34e1007db",
      "8df83df41b67444fa898dfe8aae339bd",
      "52bdd0e4ad474810891736d96f77881b",
      "7aec1a23ccd4418695993e0941359d78",
      "cd6ba500041940edb14a5b40756bafb6",
      "3b8f047e5c14453f81abdc0938e1ec85",
      "d44bb7e1f54a4d298b9b7b5e5c64fe55",
      "92326213b7da470eaaba95f0b5c93e97",
      "8d3c5b75fa004dcb88f8657af509961d",
      "da8e2242c67749ec86b34ff3b47aad41",
      "a86fa8add6f74393812be26cd74adcd1",
      "b714f1cf751e47be8feedc21a67e4ba9",
      "a8ced51a330f4331bcf254f897fb208b",
      "dfab332cf0584ca48be1bb77992e85a8",
      "2113c0b7475e4244b922c754206ceef4",
      "37e6d5a9d71a45899fe56d4a42f03dd2",
      "22660723ac584f469e354b58e02b1440",
      "1fbe47d213ca479c91fa0f8aeb75d58e",
      "f89705ceac524fccb978feb765bd0da0",
      "6197728e1854466883b521cc97d3170b",
      "e136ed17996e4e49acde352f6b359727",
      "8fabcd5be9084f2b82073caa95ef61f0",
      "001febbfa88e4167961fec1fc2ecb903",
      "9de221e779004e6aa95850b1e6f488c0",
      "a9e17fabfcba4f53bb0f9d6c513e47e9",
      "5cd2ed47e5da4710a1f90bb2e15e6cbf",
      "81a29d12b76b4a8ea73a8a376da6652e",
      "f35da49a3da44688b97c9920b2e7be72",
      "8d03d5c6b46c41b7884905b90929d001",
      "f62bc3d487db4e9884b5b128f95778bb",
      "997b5929ae3d4f42b41c7fbf03354def",
      "d537a7df77494e00b298721c380cc628",
      "2edc953cb45248e68ee928bd8e56b568",
      "6c8e6c8673894fe6b40a4d72c69b1fe5",
      "818f1bcb91ba4e30a431bf8c101df880",
      "220e09139df74cdbbe6cb87b93e151cd",
      "7db55d195d034aaa87d834fbbc5ce85e",
      "4d913c23c1fa4783bcb4403e6299bd47",
      "63845070bdfe45cab2fbee1fe9447d9a",
      "710f42a0f0fc4025ad91e95b4e2cc0a1",
      "9081783086f2434196d7331bb3b43119",
      "f00b74fdc5c640658517e94ab65a35cf",
      "0438ab7372cc4cdfb557b7f4ddd26424",
      "99683ef055d6447dbd315ac539daffbf",
      "2a514cc83b0546c8b777398309cf1f8d",
      "f244b7d2931746818623c726233f9a86",
      "34bd44a828ed4f5a9400a6913d2fbd2b",
      "7e063d02af9b4e8d82ce97ebd02c8240",
      "906dae35541a454a9005228a6808f04a",
      "837935228853484c9919e8717f727031",
      "66c5a630c87f4d459b6ee5430654e493",
      "475dba7b6694462ca08b6ff9f36cd541",
      "699c535e815a480abbd7984c090b9561",
      "600e5d779312435c82e830b3bc3eb648",
      "0a9557c188cd4285acdc82ed5a005e06",
      "b468e936ebe643d581d2b5dc082a482d",
      "e0871e308673416795f9c4b2d28224db",
      "08f1fafd53d84c21bcc011fabffc2257",
      "c11d90c4ecce43ef96a68db495c27b10",
      "39fe7022036a403ebf6ecc05373b840c",
      "3280a84e43af4cda990d61d43c0afeb8",
      "045e78feb60b4d1492e78f37b2d0cc43",
      "0955ca5055e34fc1ad1a0e1e6ab266b7",
      "45e2ef093944412b8788048245026ec9",
      "03b4c1cd94aa48f6a865ad255c419ca8",
      "1904c3b44b5b46ed958f81b087f41ea7",
      "2b535bceb1144d5eb77ae1b9a1905e2c",
      "fe058fa3b4354043831575a991b0764a",
      "97780c6144924a869e390b4c6dc8d53b",
      "0da29ca666e141d28560972f6fe2321f",
      "14b689a678c04aa3bc86713e75a1f0bb",
      "6f7f364d66ca4019b5941f81f08b33f0",
      "59b269691d7342e2b0bb6cdc1c61d574",
      "d6022f86c67e4556938c8b6da4a51700",
      "c95ed2d32c4e4eccac2bd2407e594cba",
      "36fe289508814b3180d56ba3897accbe",
      "50053bc7fcff454d85c2d2cf7c6bc202",
      "1e3ed0643b7f4c36aa1de04d8c642d6d",
      "7b766ac318a9402a91b8e0c8bc68b1e6",
      "40a6c37cf7934d27b11afea93f4e3978",
      "d79e638b11394786885456ca11a543d6",
      "64f011b38f5c48598cc8ff52159d2361",
      "378e702a26e04b03978021bd073b4b73",
      "647e77078eac4e08906619f74b0cfcd4",
      "949b06c01997495b8dcc812040d2943d",
      "d2dfe549da9243f38178e6c6bfbd18af",
      "90c96d1ae8854c8f8b9bf249f4bf6fe7",
      "c4e7d3276d5444e69a4584d08d04d4ef",
      "f9ea6a20c93e4917b988039a9b84343d",
      "2a77630a0e744d2299ca6aee2fd73c56",
      "49ba16f1e1364ddcb37290d54b24cfb3",
      "9f7aa1e517df464b8e08a26e4714fe68",
      "578931bafe9f4f82bac2981e977f7aa0",
      "fa18edd2422046bca89e124970300b31",
      "078aaaf497f64ece964db5ce7b9f05ad",
      "d80c2994a8034e0a88b9ae7cad84d368",
      "1ace3406b0da4a6cb5b40a9610dce0b8",
      "6742aa68a7004f39bd54f1828285cda8",
      "a8c34bda253b4ad89b31125f33710a93",
      "c1ecb0b84ed344d59fdcb850fd6db8c9",
      "0ba00e5405df491cbeb1d210d2da9e08",
      "43bebf5192ef4afe895b93b2a9980457",
      "2ae9e5959c93447d902dccc1b61515f0",
      "56502ff26614467db3eb6ffad049a08d",
      "b59096596e404b53ae51710bf7d5a8f2",
      "2ef831323db046a78adf4672c4f5bb7f",
      "f5896cc3a303435aa8e87b3249666ee3",
      "be47faa573564874ad15d890a0bce96e",
      "0357804a5a124934a219a8d95b9581b8",
      "45be5d3bbbe04f4aa70e00f460f0322c",
      "e1f3f5082e394a07b730d8464e184a62",
      "427a18827de946088092d49d17762256",
      "7bab0bba4f804285bc687c195cff4e43",
      "73d8a675ae5645649f926d6b02ec02bf",
      "a7e0a9b27973418f85a3c3fed77bcc54",
      "a98ed6cfbf3f44f387f35f4ace0f599d",
      "05177eafef2c4c18a430d899bafcbf50",
      "8c9250f2c86041cf884321c508eb27d5",
      "5ca3966c2a774577bed9258557a9bf8c",
      "e10d8d06849b4d678460b52ccb05e988",
      "f1b1664017af4550ac036b37fdfa26c0",
      "de48f24338374b16ac5214b1ec10e4b5",
      "416eb4c6b7084d3c9741d458b62b952f",
      "fd6919cc787045db8a07296dc9f58e85",
      "4e4993b3cd9c4a178b310e463d6ccf06",
      "6ef406b4bc434e6d95769a19be21b52a",
      "2f2471367a5c4f5a889e9a3341b29411",
      "7db9db0f8dfa4e3c923e2db9bd7ff646",
      "e851e3d8b2014480b86bf874a310b994",
      "2ce1bf733235408f842386bd151a788a",
      "3d75a44803f542ba92f18703b4813263",
      "37dfbd68384c416aa0f13eaf7cc8c7af",
      "52ea45d9ffc043d58677af379d6ac8fe",
      "39eef63f10494d38aa2929a74b408adb",
      "d72aaf48ac4549df8a32782d539eea8e",
      "2a8d0b4aaa3a4dbe8e36bc21dcd80ad7",
      "a741c6baf750433ba63c889e61d09317",
      "12e524a1947f4d14bdc5b144b80c074d",
      "1509eb38e9eb4462b0c64f06bc43dbf1",
      "4d10fe5bb9dd451bb14c4799a47fc7f7",
      "1a0fd192d6be4e4dbdd5d31119739445",
      "51e0c3e37765468eac71901e8335d0a5",
      "cf24a2f431e240648f362b2be4bf85b8",
      "3c137133ad0c4400afbea8ef05f51750",
      "708b41f5125340219c285e77b0dcc9bb",
      "4a059ffc0acd46b0ac1eee9ec6fa2489",
      "87cb380559ef419288b567ae61a35ceb",
      "b3c277c24c3d46289017533dfe220e38",
      "bccb1b80c7c5446593fb4a58aadd36e7",
      "6942b76bf94941f3940d5c91fe1a704f",
      "bec0758f80fe4a6e86cec356cdb27232",
      "ab95a8583d224dc9af17233acc78d2e1",
      "7de9f308746d44baa1c07e46dd5dd6e7",
      "8bfbb1a2c9d04eabbbe79a78d1ef30f4",
      "6db03e4494d14211a36a180d0d8764b9",
      "ac41b730c1ec4e90bedbcc62ce7a60ab",
      "33896a15450b44fd97375e0e3315d22c",
      "63f5a1e410ad4c3e98dbeb4d1efb41b4",
      "1521723f57464978af5b65d9a5daaf5f",
      "a65ca7e50432420683b3ef888dad2972",
      "c784798077a34ad18f502868e62d68b6",
      "67442ebaa9e049efa3c0303c3448526e",
      "bd4a91e710d748c6a03ef58bd7c7108e",
      "da05c1c2e3a5445e94882705ec8b9ab1",
      "e4020e11169f49309329b51094cfe5c1",
      "de49bd1921fc4c81b82478aa61884aa3",
      "0631b9b0a37043189a0a12d681114399",
      "9b8affc52442434d938e5267aae14974",
      "ce1b6a20f93c439b87f3cdc5ae79d2ec",
      "287299f4db1e47a9a52ef6c1c823426b",
      "8dc2444ff31643a89ae2db62410b9304",
      "fc75e7f6127d4a24a6130e90ce168219",
      "ea1fee8d30ae4af4958022b65099ada8",
      "3446ff0acea24f2394b6e8d6e358a806",
      "aa0cb21943ec4e8485ee9a8ccc26f6c5",
      "77dd98c9903f4b47bddb11a3686f8c30",
      "3dd8a3591fd0483f9e039daf0ac65329",
      "59d42c87252a4700b6969945c276a846",
      "447bbe7c4eb04e37a3563effc5c1a11d",
      "0e2c6c79cd714b469d12d845675888f5",
      "d47cdfe7f1fa4bba8d1495ca19bf818a",
      "5975d442b0b7428fb308e44a23f5cbd8",
      "36882e4415a944ee811adc82bfffdb8a",
      "6af7832dd1a049b595fb4d15e224658f",
      "c12f1a3cb6654220936c00a7dc4204d4",
      "07a4a96270d2480fbdbef292eebe5940",
      "915cd364219d4fa38d9937be3542159f",
      "f15ea3a1f99b4f7e8f37849959ec0c65",
      "6501b1e8a805471d9de10f47498a5464",
      "c8623c587dce4c6ebffb23eef542cc8d",
      "4a8cb3b32dd4450a877f74a0583d7632",
      "f93cae5ac5984bee91cb9d656cfe0340",
      "98103481f9564b3c82fbc907d4b2c373",
      "314eab322a2e4f09894791d682e679eb",
      "c117b262ec534319baaf9bc01d94fa6c",
      "d51e7c69d3334bdda769a25173ae77ec",
      "ab40bbc2953542a4bafd91c1b362bd4a",
      "5b48333b03f8459b8a4f3c7e68c3677a",
      "778e9668fdaf4f668df3d750ecc069a6",
      "5155b6bf949b4a979ff62647b590aa22",
      "26f0177469344a23b2bf37131662472f",
      "467fe454bdfe44eca8962cf1b7e9fa89",
      "25473fdc98dd4f60adfd5ea79e933047",
      "da3eb41ddcc54d2887aa5019c27948a6",
      "c47a8ad424594c5eb8412a8875f766b9"
     ]
    },
    "id": "yae4GJUbgvCz",
    "outputId": "bfbb0fdc-03d6-43d9-8f20-dec5de952046"
   },
   "outputs": [],
   "source": [
    "setup_llm_and_embedding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4lIikNPXdcxT",
    "outputId": "2b9b654a-0e76-435a-a3ac-194d5c2d4d8c"
   },
   "outputs": [],
   "source": [
    "\n",
    "db = chromadb.PersistentClient(path=INDEX_STORAGE)\n",
    "col = db.get_collection(COLLECTION_NAME)\n",
    "print(\"üì¶ S·ªë l∆∞·ª£ng vectors trong Chroma:\", col.count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nHVVlHG4xQq9"
   },
   "outputs": [],
   "source": [
    "def print_page(page_number):\n",
    "    print(f\"\\n==================== PAGE {page_number} ====================\\n\")\n",
    "\n",
    "    db = chromadb.PersistentClient(path=INDEX_STORAGE)\n",
    "    col = db.get_collection(COLLECTION_NAME)\n",
    "\n",
    "    results = col.get(\n",
    "        where={\"page_label\": str(page_number)},\n",
    "        include=[\"documents\", \"metadatas\"]\n",
    "    )\n",
    "\n",
    "    if not results[\"documents\"]:\n",
    "        print(\"‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y trang n√†y trong Chroma.\")\n",
    "        return\n",
    "\n",
    "    text = results[\"documents\"][0]\n",
    "    meta = results[\"metadatas\"][0]\n",
    "\n",
    "    # In text\n",
    "    print(\"üìÑ **N·ªòI DUNG TRANG:**\")\n",
    "    print(\"-\" * 60)\n",
    "    print(text[:1500])\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "    # In metadata\n",
    "    print(\"\\nüîñ **METADATA:**\")\n",
    "    for key, val in meta.items():\n",
    "        print(f\"  ‚Ä¢ {key}: {val}\")\n",
    "\n",
    "    # üî¢ Th√™m ph·∫ßn ƒë·∫øm ƒë·ªô d√†i\n",
    "    print(\"\\nüî¢ **ƒê·ªò D√ÄI DOCUMENT:**\")\n",
    "    print(f\"  ‚Ä¢ S·ªë k√Ω t·ª±: {len(text)}\")\n",
    "    print(f\"  ‚Ä¢ S·ªë t·ª´: {len(text.split())}\")\n",
    "\n",
    "    print(\"\\n============================================================\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nhrGDfKjxpmA",
    "outputId": "72e62c99-1933-46d7-b2a2-dd46d479a2f8"
   },
   "outputs": [],
   "source": [
    "print_page(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J8N9CdV9wJGO"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "from llama_index.retrievers.bm25 import BM25Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H9f2GGt9UYge"
   },
   "outputs": [],
   "source": [
    "def should_use_rag(message: str, last_user_message: str = \"\") -> bool:\n",
    "    print(\"üß† [Gate] ƒêang ph√¢n t√≠ch √Ω ƒë·ªãnh (Expanded)...\")\n",
    "\n",
    "    msg = message.lower().strip()\n",
    "    last_msg = last_user_message.lower().strip()\n",
    "\n",
    "    # =================================================================\n",
    "    # üö® NH√ìM 0: KH·ª¶NG HO·∫¢NG & AN TO√ÄN (CRISIS)\n",
    "    # C√°c t·ª´ kh√≥a n√†y b√°o hi·ªáu nguy hi·ªÉm -> C·∫ßn x·ª≠ l√Ω ngay (Force RAG ho·∫∑c Alert)\n",
    "    # =================================================================\n",
    "    crisis_keywords = [\n",
    "        \"t·ª± t·ª≠\", \"t·ª± s√°t\", \"suicidal\", \"mu·ªën ch·∫øt\", \"k·∫øt th√∫c cu·ªôc ƒë·ªùi\",\n",
    "        \"r·∫°ch tay\", \"t·ª± h·∫°i\", \"self-harm\", \"nh·∫£y l·∫ßu\", \"u·ªëng thu·ªëc ng·ªß\",\n",
    "        \"gi·∫øt ng∆∞·ªùi\", \"h·∫°i ng∆∞·ªùi\", \"kh√¥ng mu·ªën s·ªëng n·ªØa\", \"tuy·ªát v·ªçng\"\n",
    "    ]\n",
    "\n",
    "    if any(k in msg for k in crisis_keywords):\n",
    "        print(\"üß† [Gate] ‚ö†Ô∏è PH√ÅT HI·ªÜN T√çN HI·ªÜU KH·∫®N C·∫§P.\")\n",
    "        return True\n",
    "\n",
    "    # =================================================================\n",
    "    # üìö NH√ìM 1: T√äN R·ªêI LO·∫†N (DISORDERS) - M·ªü r·ªông theo ch∆∞∆°ng DSM-5\n",
    "    # =================================================================\n",
    "    disorders = [\n",
    "        # --- T√¢m tr·∫°ng & Lo √¢u ---\n",
    "        \"tr·∫ßm c·∫£m\", \"depression\", \"u u·∫•t\", \"kh√≠ s·∫Øc\",\n",
    "        \"l∆∞·ª°ng c·ª±c\", \"bipolar\", \"h∆∞ng c·∫£m\", \"mania\", \"hypomania\",\n",
    "        \"lo √¢u\", \"anxiety\", \"ho·∫£ng lo·∫°n\", \"panic\", \"s·ª£ x√£ h·ªôi\", \"√°m ·∫£nh s·ª£\",\n",
    "\n",
    "        # --- OCD & Stress ---\n",
    "        \"ocd\", \"√°m ·∫£nh c∆∞·ª°ng b·ª©c\", \"nghi th·ª©c\", \"t√≠ch tr·ªØ\", \"hoarding\",\n",
    "        \"ptsd\", \"sang ch·∫•n\", \"h·∫≠u ch·∫•n th∆∞∆°ng\", \"stress c·∫•p t√≠nh\", \"th√≠ch ·ª©ng\",\n",
    "\n",
    "        # --- Lo·∫°n th·∫ßn (Psychotic) ---\n",
    "        \"t√¢m th·∫ßn ph√¢n li·ªát\", \"schizophrenia\", \"hoang t∆∞·ªüng\", \"·∫£o gi√°c\",\n",
    "        \"nghe ti·∫øng n√≥i\", \"lo·∫°n th·∫ßn\", \"psychosis\", \"catatonia\", \"cƒÉng tr∆∞∆°ng l·ª±c\",\n",
    "\n",
    "        # --- Ph√°t tri·ªÉn th·∫ßn kinh ---\n",
    "        \"t·ª± k·ª∑\", \"autism\", \"asd\", \"tƒÉng ƒë·ªông\", \"gi·∫£m ch√∫ √Ω\", \"adhd\",\n",
    "        \"khi·∫øm khuy·∫øt tr√≠ tu·ªá\", \"ch·∫≠m ph√°t tri·ªÉn\", \"tik\", \"tourette\",\n",
    "\n",
    "        # --- ƒÇn u·ªëng (Feeding & Eating) ---\n",
    "        \"ch√°n ƒÉn\", \"anorexia\", \"ƒÉn v√¥ ƒë·ªô\", \"bulimia\", \"binge eating\", \"pica\",\n",
    "\n",
    "        # --- Nh√¢n c√°ch (Personality) ---\n",
    "        \"r·ªëi lo·∫°n nh√¢n c√°ch\", \"borderline\", \"ranh gi·ªõi\", \"√°i k·ª∑\", \"narcissistic\",\n",
    "        \"ch·ªëng ƒë·ªëi x√£ h·ªôi\", \"antisocial\", \"tr√°nh n√©\", \"ph·ª• thu·ªôc\", \"ƒëa nh√¢n c√°ch\",\n",
    "\n",
    "        # --- Gi·∫•c ng·ªß & Kh√°c ---\n",
    "        \"m·∫•t ng·ªß\", \"insomnia\", \"ng·ªß r≈©\", \"narcolepsy\", \"√°c m·ªông\",\n",
    "        \"nghi·ªán\", \"cai nghi·ªán\", \"ma t√∫y\", \"r∆∞·ª£u\", \"ch·∫•t k√≠ch th√≠ch\",\n",
    "        \"r·ªëi lo·∫°n t√¨nh d·ª•c\", \"lo·∫°n d·ª•c\", \"gi·ªõi t√≠nh\", \"dysphoria\",\n",
    "        \"m·∫•t tr√≠ nh·ªõ\", \"alzheimer\", \"gi·∫£ b·ªánh\"\n",
    "    ]\n",
    "\n",
    "    # =================================================================\n",
    "    # ü©∫ NH√ìM 2: TRI·ªÜU CH·ª®NG & BI·ªÇU HI·ªÜN (SYMPTOMS)\n",
    "    # Nh·ªØng t·ª´ m√¥ t·∫£ tr·∫°ng th√°i b·ªánh l√Ω c·ª• th·ªÉ\n",
    "    # =================================================================\n",
    "    symptoms = [\n",
    "        \"·∫£o thanh\", \"·∫£o ·∫£nh\", \"m·∫•t ki·ªÉm so√°t\", \"b·ªëc ƒë·ªìng\", \"v√¥ c·∫£m\",\n",
    "        \"k√≠ch ƒë·ªông\", \"g√¢y g·ªï\", \"thu m√¨nh\", \"s·ª£ ƒë√°m ƒë√¥ng\", \"r·ª≠a tay li√™n t·ª•c\",\n",
    "        \"ki·ªÉm tra li√™n t·ª•c\", \"nh·ªõ l·∫°i\", \"flashback\", \"√°c m·ªông\",\n",
    "        \"m·ªát m·ªèi kinh ni√™n\", \"s·ª•t c√¢n\", \"tƒÉng c√¢n\", \"m·∫•t ng·ªß k√©o d√†i\",\n",
    "        \"tim ƒë·∫≠p nhanh\", \"kh√≥ th·ªü\", \"ng·∫•t\", \"run tay\", \"v√£ m·ªì h√¥i\",\n",
    "        \"tr·ªëng r·ªóng\", \"b·ªè r∆°i\", \"ƒëa nghi\", \"ghen tu√¥ng hoang t∆∞·ªüng\"\n",
    "    ]\n",
    "\n",
    "    # =================================================================\n",
    "    # üîç NH√ìM 3: √ù ƒê·ªäNH TRA C·ª®U (DIAGNOSTIC INTENT)\n",
    "    # C√°c t·ª´ kh√≥a th·ªÉ hi·ªán user mu·ªën t√¨m ki·∫øn th·ª©c\n",
    "    # =================================================================\n",
    "    diagnostic_intent = [\n",
    "        \"ti√™u chu·∫©n\", \"d·∫•u hi·ªáu\", \"bi·ªÉu hi·ªán\", \"tri·ªáu ch·ª©ng\",\n",
    "        \"ch·∫©n ƒëo√°n\", \"diagnose\", \"criteria\", \"x√©t nghi·ªám\",\n",
    "        \"dsm\", \"dsm-5\", \"icd\", \"m√£ b·ªánh\",\n",
    "        \"c√≥ ph·∫£i l√†\", \"c√≥ ph·∫£i b·ªã\", \"t√¥i b·ªã g√¨\", \"b·ªánh g√¨\",\n",
    "        \"ph√¢n bi·ªát\", \"kh√°c nhau\", \"nguy√™n nh√¢n\", \"y·∫øu t·ªë nguy c∆°\",\n",
    "        \"th·ªùi gian k√©o d√†i\", \"bao l√¢u th√¨\", \"ti√™n l∆∞·ª£ng\", \"y·∫øu t·ªë\"\n",
    "    ]\n",
    "\n",
    "    # --- LOGIC KI·ªÇM TRA ---\n",
    "\n",
    "    # 1. Check tr·ª±c ti·∫øp trong tin nh·∫Øn hi·ªán t·∫°i\n",
    "    # G·ªôp t·∫•t c·∫£ keywords ƒë·ªÉ check 1 l·∫ßn cho nhanh\n",
    "    all_keywords = disorders + symptoms + diagnostic_intent\n",
    "\n",
    "    # D√πng v√≤ng l·∫∑p check t·ª´ng t·ª´ (c√≥ th·ªÉ t·ªëi ∆∞u b·∫±ng regex n·∫øu c·∫ßn ch√≠nh x√°c tuy·ªát ƒë·ªëi)\n",
    "    for kw in all_keywords:\n",
    "        if kw in msg:\n",
    "            print(f\"üß† [Gate] C·∫¶N RAG (Keyword: '{kw}').\")\n",
    "            return True\n",
    "\n",
    "    # 2. Check Context (C√¢u tr∆∞·ªõc ƒë√≥ c√≥ n√≥i v·ªÅ b·ªánh kh√¥ng?)\n",
    "    # N·∫øu c√¢u tr∆∞·ªõc c√≥ keyword b·ªánh -> C√¢u n√†y kh·∫£ nƒÉng cao l√† follow-up\n",
    "    context_keywords = disorders + symptoms # Ch·ªâ quan t√¢m t√™n b·ªánh/tri·ªáu ch·ª©ng ·ªü context\n",
    "    has_medical_context = any(kw in last_msg for kw in context_keywords)\n",
    "\n",
    "    if has_medical_context:\n",
    "        # Danh s√°ch t·ª´ ƒë·ªÉ NG·∫ÆT RAG (n·∫øu user mu·ªën d·ª´ng)\n",
    "        stop_words = [\"c·∫£m ∆°n\", \"ok\", \"ƒë∆∞·ª£c r·ªìi\", \"hi·ªÉu r·ªìi\", \"bye\", \"t·∫°m bi·ªát\", \"kh√¥ng sao\"]\n",
    "        if any(w == msg for w in stop_words):\n",
    "            print(\"üß† [Gate] Context c√≥ b·ªánh, nh∆∞ng User d·ª´ng -> KH√îNG RAG.\")\n",
    "            return False\n",
    "\n",
    "        # N·∫øu kh√¥ng ph·∫£i t·ª´ d·ª´ng, m√† context ƒëang n√≥i v·ªÅ b·ªánh -> Ti·∫øp t·ª•c tra c·ª©u\n",
    "        print(\"üß† [Gate] FORCE RAG (Theo ng·ªØ c·∫£nh h·ªôi tho·∫°i c≈©).\")\n",
    "        return True\n",
    "\n",
    "    # 3. N·∫øu kh√¥ng d√≠nh keyword n√†o -> Small Talk / General Chat\n",
    "    print(\"üß† [Gate] Kh√¥ng t√¨m th·∫•y y·∫øu t·ªë chuy√™n m√¥n -> KH√îNG RAG.\")\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N2kRxUOcUaGT"
   },
   "outputs": [],
   "source": [
    "# B·∫°n c·∫ßn import traceback n·∫øu ch∆∞a c√≥:\n",
    "import traceback\n",
    "\n",
    "def setup_chatbot_environment():\n",
    "    \"\"\"Load Embedding + LLaMA + Chroma + Hybrid RAG\"\"\"\n",
    "    global memory, reranker_model, vector_index_chat, all_nodes\n",
    "    global bm25_retriever, semantic_retriever, fusion_retriever, llm_chat\n",
    "    global embed_model, llm  # ƒê·∫£m b·∫£o bi·∫øn phase 1 truy c·∫≠p ƒë∆∞·ª£c\n",
    "\n",
    "    try:\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"üîπ GIAI ƒêO·∫†N 2: SETUP CHATBOT HYBRID (sau reset kernel)\")\n",
    "        print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "        # ---------------------------------------------------------\n",
    "        # üî• KI·ªÇM TRA ‚Äì Giai ƒëo·∫°n 1 ƒë√£ ch·∫°y ch∆∞a?\n",
    "        # ---------------------------------------------------------\n",
    "        if 'embed_model' not in globals() or embed_model is None:\n",
    "            print(\"‚ùó embed_model ch∆∞a ƒë∆∞·ª£c load. H√£y ch·∫°y setup_llm_and_embedding() tr∆∞·ªõc!\")\n",
    "            return False\n",
    "\n",
    "        if 'llm' not in globals() or llm is None:\n",
    "            print(\"‚ùó llm ch∆∞a ƒë∆∞·ª£c load. H√£y ch·∫°y setup_llm_and_embedding() tr∆∞·ªõc!\")\n",
    "            return False\n",
    "\n",
    "        # 1) Embedding cho query\n",
    "        Settings.embed_model = embed_model\n",
    "\n",
    "        # 2) LLaMA l√†m LLM chat\n",
    "        llm_chat = llm\n",
    "        Settings.llm = llm_chat\n",
    "\n",
    "        # ---------------------------------------------------------\n",
    "        # 3) K·∫øt n·ªëi ChromaDB\n",
    "        # ---------------------------------------------------------\n",
    "        print(f\"‚è≥ ƒêang k·∫øt n·ªëi ChromaDB t·∫°i: {INDEX_STORAGE}\")\n",
    "        db = chromadb.PersistentClient(path=INDEX_STORAGE)\n",
    "        chroma_collection = db.get_collection(COLLECTION_NAME)\n",
    "\n",
    "        print(\"üì¶ S·ªë l∆∞·ª£ng vectors trong Chroma:\", chroma_collection.count())\n",
    "\n",
    "        vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "        storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "\n",
    "        # ---------------------------------------------------------\n",
    "        # 4) Load VectorStoreIndex t·ª´ Chroma\n",
    "        # ---------------------------------------------------------\n",
    "        vector_index_chat = VectorStoreIndex.from_vector_store(\n",
    "            vector_store=vector_store,\n",
    "            embed_model=embed_model,\n",
    "        )\n",
    "        print(\"‚úÖ Vector Index ƒë√£ loaqud t·ª´ Chroma.\\n\")\n",
    "\n",
    "        # ---------------------------------------------------------\n",
    "        # 5) L·∫§Y NODES TR·ª∞C TI·∫æP T·ª™ CHROMA (KH√îNG D√ôNG docstore)\n",
    "        # ---------------------------------------------------------\n",
    "        print(\"‚è≥ ƒêang l·∫•y nodes t·ª´ Chroma collection...\")\n",
    "        results = chroma_collection.get(\n",
    "            include=[\"metadatas\", \"documents\"]\n",
    "        )\n",
    "\n",
    "        all_nodes = []\n",
    "        for doc, meta, _id in zip(results[\"documents\"], results[\"metadatas\"], results[\"ids\"]):\n",
    "            node = TextNode(\n",
    "                text=doc,\n",
    "                id_=_id,\n",
    "                metadata=meta or {},\n",
    "            )\n",
    "            all_nodes.append(node)\n",
    "\n",
    "        print(f\"üì¶ T·ªïng s·ªë nodes reconstruct t·ª´ Chroma: {len(all_nodes)}\\n\")\n",
    "\n",
    "        # ---------------------------------------------------------\n",
    "        # 6) BM25 Retriever\n",
    "        # ---------------------------------------------------------\n",
    "        bm25_nodes = []\n",
    "        for n in all_nodes:\n",
    "            bm25_text = n.metadata.get(\"bm25_text\", n.text)\n",
    "            n_bm25 = TextNode(\n",
    "                text=bm25_text,\n",
    "                id_=n.node_id,\n",
    "                metadata=n.metadata,\n",
    "            )\n",
    "            bm25_nodes.append(n_bm25)\n",
    "\n",
    "        print(\"‚è≥ T·∫°o BM25Retriever (lexical)...\")\n",
    "        bm25_retriever = BM25Retriever.from_defaults(\n",
    "            nodes=bm25_nodes,\n",
    "            similarity_top_k=5,\n",
    "        )\n",
    "        print(\"‚úÖ BM25Retriever s·∫µn s√†ng.\\n\")\n",
    "\n",
    "        # ---------------------------------------------------------\n",
    "        # 7) Semantic Retriever (vector)\n",
    "        # ---------------------------------------------------------\n",
    "        print(\"‚è≥ T·∫°o Semantic Retriever (vector)...\")\n",
    "        semantic_retriever = VectorIndexRetriever(\n",
    "            index=vector_index_chat,\n",
    "            similarity_top_k=5,\n",
    "        )\n",
    "        print(\"‚úÖ Semantic Retriever s·∫µn s√†ng.\\n\")\n",
    "\n",
    "        # ---------------------------------------------------------\n",
    "        # 8) Fusion Retriever\n",
    "        # ---------------------------------------------------------\n",
    "        print(\"‚è≥ T·∫°o QueryFusionRetriever (Hybrid)...\")\n",
    "        fusion_retriever = QueryFusionRetriever(\n",
    "            retrievers=[bm25_retriever, semantic_retriever],\n",
    "            similarity_top_k=5,\n",
    "            num_queries=2,\n",
    "            mode=\"reciprocal_rerank\",\n",
    "        )\n",
    "        print(\"‚úÖ Hybrid Fusion Retriever s·∫µn s√†ng.\\n\")\n",
    "\n",
    "        # ---------------------------------------------------------\n",
    "        # 9) Reranker\n",
    "        # ---------------------------------------------------------\n",
    "        print(\"‚è≥ T·∫£i Cross-Encoder Reranker...\")\n",
    "        # ƒê·∫∑t thi·∫øt b·ªã m·∫∑c ƒë·ªãnh l√† \"cpu\"\n",
    "        reranker_model = CrossEncoder(\n",
    "            \"cross-encoder/ms-marco-MiniLM-L6-v2\",\n",
    "        )\n",
    "        print(\"‚úÖ Reranker ƒë√£ s·∫µn s√†ng.\\n\")\n",
    "\n",
    "        # ---------------------------------------------------------\n",
    "        # 10) Memory + System Prompt\n",
    "        # ---------------------------------------------------------\n",
    "        SYSTEM_PROMPT = \"\"\"\\\n",
    "BB·∫°n l√† m·ªôt **chuy√™n gia t√¢m l√Ω AI** (Tr·ª£ l√Ω) ƒë∆∞·ª£c ph√°t tri·ªÉn b·ªüi **AI VIETNAM**.\n",
    "Nhi·ªám v·ª• c·ªßa b·∫°n l√† tr√≤ chuy·ªán, theo d√µi v√† t∆∞ v·∫•n cho ng∆∞·ªùi d√πng (User) v·ªÅ s·ª©c kh·ªèe t√¢m th·∫ßn.\n",
    "Lu√¥n lu√¥n h√†nh ƒë·ªông v·ªõi t∆∞ c√°ch l√† Tr·ª£ l√Ω, th·ªÉ hi·ªán s·ª± ƒë·ªìng c·∫£m v√† chuy√™n nghi·ªáp.\n",
    "\n",
    "===========================\n",
    "QUY T·∫ÆC RAG:\n",
    "===========================\n",
    "- N·∫øu User h·ªèi v·ªÅ th√¥ng tin chuy√™n m√¥n (tri·ªáu ch·ª©ng, DSM-5, r·ªëi lo·∫°n, tr·ªã li·ªáu, ph√¢n lo·∫°i ch·∫©n ƒëo√°n...),\n",
    "  b·∫°n S·∫º nh·∫≠n ƒë∆∞·ª£c m·ªôt tin nh·∫Øn System ch·ª©a th√¥ng tin tham kh·∫£o.\n",
    "- H√£y d·ª±a HO√ÄN TO√ÄN v√†o th√¥ng tin ƒë√≥ ƒë·ªÉ tr·∫£ l·ªùi.\n",
    "- Tuy·ªát ƒë·ªëi KH√îNG ƒë∆∞·ª£c t·ª± b·ªãa ki·∫øn th·ª©c khi ƒë√£ k√≠ch ho·∫°t RAG.\n",
    "\n",
    "===========================\n",
    "QUY T·∫ÆC KH√îNG RAG:\n",
    "===========================\n",
    "- Khi KH√îNG c√≥ RAG, b·∫°n tr√≤ chuy·ªán t·ª± nhi√™n nh∆∞ m·ªôt nh√† tr·ªã li·ªáu t√¢m l√Ω:\n",
    "  lu√¥n ƒë·ªìng c·∫£m, l·∫Øng nghe, ph·∫£n √°nh c·∫£m x√∫c, ƒë·∫∑t c√¢u h·ªèi m·ªü.\n",
    "- Tuy nhi√™n, h√£y **g·ª£i √Ω m·ªôt c√°ch tinh t·∫ø** ƒë·ªÉ ng∆∞·ªùi d√πng chia s·∫ª s√¢u h∆°n ‚Äî\n",
    "  nh·∫±m nh·∫≠n bi·∫øt khi n√†o n√™n chuy·ªÉn sang RAG.\n",
    "  V√≠ d·ª•:\n",
    "  - ‚ÄúB·∫°n c√≥ c·∫£m th·∫•y tri·ªáu ch·ª©ng ƒë√≥ k√©o d√†i bao l√¢u r·ªìi?‚Äù\n",
    "  - ‚Äúƒêi·ªÅu n√†y c√≥ ·∫£nh h∆∞·ªüng m·∫°nh ƒë·∫øn sinh ho·∫°t c·ªßa b·∫°n kh√¥ng?‚Äù\n",
    "  - ‚ÄúB·∫°n m√¥ t·∫£ r√µ h∆°n c·∫£m gi√°c trong nh·ªØng l√∫c nh∆∞ v·∫≠y ƒë∆∞·ª£c kh√¥ng?‚Äù\n",
    "\n",
    "  Tr√°nh h·ªèi d·ªìn d·∫≠p; ch·ªâ g·ª£i m·ªü nh·∫π nh√†ng ƒë·ªÉ nh·∫≠n di·ªán nhu c·∫ßu chuy√™n m√¥n.\n",
    "\n",
    "===========================\n",
    "QUY T·∫ÆC B·∫¢O V·ªÜ (R·∫§T QUAN TR·ªåNG):\n",
    "===========================\n",
    "- B·∫°n **TUY·ªÜT ƒê·ªêI KH√îNG** tr·∫£ l·ªùi c√°c c√¢u h·ªèi KH√îNG li√™n quan ƒë·∫øn t√¢m l√Ω ho·∫∑c s·ª©c kh·ªèe t√¢m th·∫ßn.\n",
    "- C√°c ch·ªß ƒë·ªÅ C·∫§M g·ªìm nh∆∞ng kh√¥ng gi·ªõi h·∫°n:\n",
    "  **th·ªùi ti·∫øt, n·∫•u ƒÉn, ch√≠nh tr·ªã, th·ªÉ thao, tin t·ª©c, l·∫≠p tr√¨nh, to√°n h·ªçc, c√¥ng ngh·ªá, m·∫πo ƒë·ªùi s·ªëng, gossip...**\n",
    "- Khi b·ªã h·ªèi v·ªÅ ch·ªß ƒë·ªÅ c·∫•m, h√£y l·ªãch s·ª± t·ª´ ch·ªëi v√† k√©o ng∆∞·ªùi d√πng v·ªÅ ƒë√∫ng ch·ªß ƒë·ªÅ.\n",
    "  V√≠ d·ª•:\n",
    "  ‚ÄúT√¥i xin l·ªói, t√¥i ch·ªâ ƒë∆∞·ª£c ƒë√†o t·∫°o ƒë·ªÉ h·ªó tr·ª£ v·ªÅ s·ª©c kh·ªèe t√¢m th·∫ßn.\n",
    "   B·∫°n c√≥ ƒëang g·∫∑p kh√≥ khƒÉn n√†o khi·∫øn b·∫°n mu·ªën tr√≤ chuy·ªán h√¥m nay kh√¥ng?‚Äù\n",
    "\n",
    "===========================\n",
    "PHONG C√ÅCH TR·∫¢ L·ªúI:\n",
    "===========================\n",
    "- Lu√¥n gi·ªØ gi·ªçng n√≥i:\n",
    "  **·∫•m √°p ‚Äì kh√¥ng ph√°n x√©t ‚Äì chuy√™n nghi·ªáp ‚Äì nh·∫π nh√†ng ‚Äì h∆∞·ªõng d·∫´n.**\n",
    "- Ng·∫Øn g·ªçn nh∆∞ng s√¢u s·∫Øc.\n",
    "- Kh√¥ng bao gi·ªù bi·∫øn th√†nh chatbot ƒëa nƒÉng.\n",
    "- B·∫°n l√† m·ªôt **therapist AI**, kh√¥ng ph·∫£i ‚Äútr·ª£ l√Ω h·ªèi g√¨ c≈©ng tr·∫£ l·ªùi‚Äù.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "        memory = ChatMemoryBuffer.from_defaults(token_limit=4096)\n",
    "        memory.put(ChatMessage(role=\"system\", content=SYSTEM_PROMPT))\n",
    "\n",
    "        print(\"üéâ Giai ƒëo·∫°n 2: Setup HYBRID ho√†n t·∫•t.\\n\")\n",
    "        return True\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå L·ªñI GIAI ƒêO·∫†N 2 (SETUP): {e}\")\n",
    "        traceback.print_exc()\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cyxpmP0BUbz7"
   },
   "outputs": [],
   "source": [
    "def rag_retrieve_and_rerank(message: str, top_k=3, final_top=2) -> str:\n",
    "    print(f\"üîç [RAG HYBRID] Truy v·∫•n: '{message}'\")\n",
    "\n",
    "    # 1) Hybrid retrieve\n",
    "    retrieved_docs = fusion_retriever.retrieve(message)\n",
    "    if not retrieved_docs:\n",
    "        print(\"‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y t√†i li·ªáu li√™n quan.\")\n",
    "        return \"\"\n",
    "\n",
    "    print(f\"üîç [RAG HYBRID] Fusion tr·∫£ v·ªÅ {len(retrieved_docs)} docs. ƒêang rerank...\")\n",
    "\n",
    "    # Ch·ªâ rerank t·ªëi ƒëa 6 documents ƒë·ªÉ tƒÉng t·ªëc\n",
    "    retrieved_docs = retrieved_docs[:6]\n",
    "\n",
    "    # 2) Chu·∫©n b·ªã input cho CrossEncoder\n",
    "    doc_texts = [doc.get_content() for doc in retrieved_docs]\n",
    "    pairs = [(message, text) for text in doc_texts]\n",
    "\n",
    "    # 3) Rerank\n",
    "    try:\n",
    "        scores = reranker_model.predict(pairs)\n",
    "        ranked = sorted(\n",
    "            zip(scores, retrieved_docs),\n",
    "            key=lambda x: x[0],\n",
    "            reverse=True,\n",
    "        )\n",
    "        ranked_docs = [doc for _, doc in ranked]\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Rerank l·ªói: {e}\")\n",
    "        ranked_docs = retrieved_docs\n",
    "\n",
    "    # 4) Top N cu·ªëi c√πng\n",
    "    top_docs = ranked_docs[:final_top]\n",
    "    context_text = \"\\n\\n---\\n\\n\".join([doc.get_content() for doc in top_docs])\n",
    "\n",
    "    # 5) T√≥m t·∫Øt n·∫øu qu√° d√†i\n",
    "    if len(context_text.split()) > 700:\n",
    "        print(\"‚úÇÔ∏è Context qu√° d√†i ‚Äì ƒëang t√≥m t·∫Øt (d√πng llm_chat)...\")\n",
    "        try:\n",
    "            summary_messages = [\n",
    "                ChatMessage(\n",
    "                    role=\"system\",\n",
    "                    content=(\n",
    "                        \"B·∫°n l√† AI t√≥m t·∫Øt t√†i li·ªáu DSM-5. \"\n",
    "                        \"T√≥m t·∫Øt ng·∫Øn g·ªçn, ch√≠nh x√°c, ch·ªâ gi·ªØ ph·∫ßn li√™n quan t·ªõi c√¢u h·ªèi.\"\n",
    "                    ),\n",
    "                ),\n",
    "                ChatMessage(\n",
    "                    role=\"user\",\n",
    "                    content=f\"C√¢u h·ªèi: {message}\\n\\nN·ªôi dung t√†i li·ªáu:\\n{context_text}\",\n",
    "                ),\n",
    "            ]\n",
    "            summary_response = llm_chat.chat(summary_messages)\n",
    "            context_text = summary_response.message.content.strip()\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå L·ªói t√≥m t·∫Øt: {e}\")\n",
    "            context_text = \" \".join(context_text.split()[:700])\n",
    "\n",
    "    return f\"--- Th√¥ng tin tham kh·∫£o t·ª´ DSM-5 ---\\n{context_text}\\n--- H·∫øt th√¥ng tin tham kh·∫£o ---\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 727,
     "referenced_widgets": [
      "ed421a6462f6404088aa4bc6d0a2e5f9",
      "02be62e679af42069964a2e907e48d88",
      "5c2fadc9db674a39a0cfda49629ae1cf",
      "29e2d23a6dd74081a355832275b00e44",
      "0b53e6aa1f4c4706af434632a5e26729",
      "cd831d3cd499443eb4ce27cf62cf17e2",
      "36d3f9b57f52442f833fa3b3b8331b29",
      "9babcbe2706c4000954921b842b29b9d",
      "4cd38fd9ae9e4c1993b01f52909791c1",
      "4f6abca485554633b717b480c4b75616",
      "da2b09bd1a6747d8abf1ceb4fa0f7143",
      "7d90dd0d10224142bdfe6c3bcf925f19",
      "77aa271b36774d72bac48d5284476210",
      "0052213fbec94c339f697eee71fff6d2",
      "1386342a514149369649a83a95a4a21e",
      "099eb9b46aa54436abade8000e34553c",
      "ce520d287cfe45d982de4745690b11a3",
      "d71c6fb151144b8996259a2f89b1ab1f",
      "c09951ee8a184690a1a2060b7a5b5f14",
      "3bc3c07971e04f5aa687195ee7cf481f",
      "f2e19830050843efa36f90df094ecd42",
      "d4e563e1486b4dafb2d6f6729f7b943f",
      "6936e2b282a74833a0f83ddd9fe77f35",
      "f2ce1c558af14e0a8e9b51937ad0f725",
      "ac100d98e55341229a7ce2f7daa55a24",
      "b88289cb72bd4c47aad7bbe30f069da6",
      "3f6de53f9ae74172bb0c6d7cfc801b73",
      "235880c951cf4bc188446d8c35f09e0e",
      "bb7a9e0703774b288c50f270a597dc89",
      "f39976203af440f88f80670ad2025542",
      "662fc86cf67c4d4794867ca0f5928335",
      "e33070bfb65e4a71bc1fe3ea2040d1b6",
      "504059e4f6e243ef9823fdd8c0e5d09c",
      "44d9ab600cfe448abb69a1a0c0d73771",
      "5115e54cdbea4145a044bb2f1c091c5b",
      "0d8712b70d5840dfb40d5fc40e2b9b98",
      "3990976721ce472db9818a2b8a4780bd",
      "644474e2979846e0a78d9b493e772255",
      "7e74c1fd5f094452ba3743f8fbf6dc7e",
      "b849fa0ecf3c4d5491964504baa8c0f5",
      "e2bb761c12bc41f8b9d7b987216dea91",
      "f3841f16a96c4c0a9c673c051f084529",
      "83658cbb85a541109225660e7a963c86",
      "ee453c5fc58347dab930c738add88d7f",
      "e9899fcc44584bd2afe918bd7494e46f",
      "ec049474e6a8464b9fd1b301a7f6f36e",
      "d1b33e57611f43f7864318ebfa7bb0aa",
      "cfe1ea68b8b34ea2a35b20a9bce2a9fd",
      "64b2d8baa7c3407e952c20b4f54c8417",
      "4d2d11e4afea477a902b5f10c15ba99b",
      "75fcadf9346d4635b3f79f7de4d371ff",
      "ab254d1337c2484c904e9b30f2e4e390",
      "6b17007d273a4561b9345b2f99919865",
      "86568c4ef9f944babc4f9cfd43babf16",
      "73c1413554e94c3d8221eacc1966c44d",
      "7c89f787c81944ea9ff99e79bbb232bc",
      "6891c3cf8c844c07b3b3f999de7c2820",
      "34505c2321c247df8e7a8a897b1be73a",
      "e0f2121a61f8470e8c5e24ef713e9ab6",
      "401adb2694954831a85b173c0481a7c0",
      "b36d2812801947faaf41f72c4a505361",
      "f325a99009744f1790e6bf6dbcc956af",
      "6a2cb225c26a4f3e84fb719bb0bb9a48",
      "a9abd57c0c1e4e98ae915220d92fb170",
      "1cb7740909eb414dad8ab8807cbf56a7",
      "13cda83dd8f94862aa094419c1699983",
      "3cf05c39e803458eb903dba74767da96",
      "e61d471803164e31b625fa637f53c0ec",
      "e55391e7ec7949f391e5a0f204b22c71",
      "bf8c0ce6c7164c9384f50adf45ce6391",
      "0dc719bd75344e05b1c9d8bd6ef25efe",
      "c4addd7bc29b4df380425036d2c827c8",
      "ba1a082f732248c6acb38bb53935784b",
      "294e3b164ef0478bbca67811c8c7e2d0",
      "256a60c47c6748f3aaf7ffb16ac75014",
      "e341cd2b3675450a804e10d8449b3aed",
      "f93617575b044f99a247551a3a9d2528"
     ]
    },
    "id": "i9EE43V0Ucte",
    "outputId": "9109c91b-0c90-4179-bb3c-0c198f3a24a8"
   },
   "outputs": [],
   "source": [
    "setup_chatbot_environment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NwB6LI3wBmfy"
   },
   "outputs": [],
   "source": [
    "# =====================================================\n",
    "# üéØ RESET MEMORY ‚Äì CHU·∫®N H√ìA CHO ChatMemoryBuffer\n",
    "# =====================================================\n",
    "def reset_memory():\n",
    "    \"\"\"\n",
    "    Reset b·ªô nh·ªõ h·ªôi tho·∫°i cho ChatMemoryBuffer trong LlamaIndex.\n",
    "    D·ª±a ƒë√∫ng c·∫•u tr√∫c memory.__dict__ m√† b·∫°n ƒëang d√πng.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        memory.chat_store.store[\"chat_history\"] = []\n",
    "        print(\"üíæ B·ªô nh·ªõ ƒë√£ ƒë∆∞·ª£c reset.\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Kh√¥ng th·ªÉ reset b·ªô nh·ªõ: {e}\")\n",
    "\n",
    "\n",
    "# =====================================================\n",
    "# üéØ H√ÄM G·ªåI MODEL (TI·ªÜN √çCH)\n",
    "# =====================================================\n",
    "def call_llm(messages):\n",
    "    \"\"\"Helper g·ªçi LLaMA + sanitize output.\"\"\"\n",
    "    response = llm_chat.chat(messages)\n",
    "    text = response.message.content.strip()\n",
    "\n",
    "    if text.lower().startswith(\"assistant\"):\n",
    "        text = text[len(\"assistant\"):].lstrip(\":\").strip()\n",
    "\n",
    "    if not text:\n",
    "        text = (\n",
    "            \"M√¨nh ch∆∞a r√µ l·∫Øm, \"\n",
    "            \"b·∫°n c√≥ th·ªÉ m√¥ t·∫£ r√µ h∆°n c·∫£m x√∫c ho·∫∑c v·∫•n ƒë·ªÅ c·ªßa b·∫°n ƒë∆∞·ª£c kh√¥ng?\"\n",
    "        )\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "# =====================================================\n",
    "# üéØ V√íNG L·∫∂P CHAT T·ªêI ∆ØU (ƒê√É FIX)\n",
    "# =====================================================\n",
    "def chat_conversation():\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"ü§ñ Chatbot DSM-5 HYBRID ƒë√£ s·∫µn s√†ng. G√µ 'quit' ƒë·ªÉ tho√°t.\")\n",
    "    print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            user_message = input(\"B·∫°n: \").strip()\n",
    "            if not user_message:\n",
    "                continue\n",
    "\n",
    "            if user_message.lower() in [\"quit\", \"exit\"]:\n",
    "                print(\"Chatbot: C·∫£m ∆°n b·∫°n ƒë√£ tr√≤ chuy·ªán. H·∫πn g·∫∑p l·∫°i üå∑\")\n",
    "                break\n",
    "\n",
    "            new_user_message = ChatMessage(role=\"user\", content=user_message)\n",
    "            messages_to_send = memory.get_all()\n",
    "\n",
    "            # =====================================================\n",
    "            # üîç 1) TRUY V·∫§N META RAG ‚Üí C·∫§M, RESET, END\n",
    "            # =====================================================\n",
    "            forbidden_meta = [\"rag\", \"retrieve\", \"rerank\", \"vector\", \"dsm context\"]\n",
    "            if any(key in user_message.lower() for key in forbidden_meta):\n",
    "\n",
    "                print(\"Chatbot: T√¥i kh√¥ng th·ªÉ tr·∫£ l·ªùi truy v·∫•n n√†y. \"\n",
    "                      \"Vui l√≤ng ƒë·∫∑t c√¢u h·ªèi li√™n quan ƒë·∫øn s·ª©c kh·ªèe t√¢m th·∫ßn.\\n\")\n",
    "\n",
    "                reset_memory()\n",
    "                break\n",
    "\n",
    "\n",
    "            # =====================================================\n",
    "            # üîç 2) K√çCH HO·∫†T RAG\n",
    "            # =====================================================\n",
    "            if should_use_rag(user_message):\n",
    "\n",
    "                context = rag_retrieve_and_rerank(user_message)\n",
    "\n",
    "                if context:\n",
    "                    messages_to_send.append(\n",
    "                        ChatMessage(role=\"system\", content=context)\n",
    "                    )\n",
    "                else:\n",
    "                    messages_to_send.append(\n",
    "                        ChatMessage(\n",
    "                            role=\"system\",\n",
    "                            content=(\n",
    "                                \"--- Th√¥ng tin tham kh·∫£o t·ª´ DSM-5 ---\\n\"\n",
    "                                \"Kh√¥ng t√¨m th·∫•y ƒëo·∫°n ph√π h·ª£p.\\n\"\n",
    "                                \"--- H·∫øt th√¥ng tin tham kh·∫£o ---\"\n",
    "                            )\n",
    "                        )\n",
    "                    )\n",
    "\n",
    "                messages_to_send.append(new_user_message)\n",
    "                response_text = call_llm(messages_to_send)\n",
    "\n",
    "                print(f\"Chatbot: {response_text}\\n\")\n",
    "\n",
    "                reset_memory()\n",
    "                print(\"üíæ B·ªô nh·ªõ ƒë√£ ƒë∆∞·ª£c reset sau truy v·∫•n RAG.\\n\")\n",
    "                continue\n",
    "\n",
    "\n",
    "            # =====================================================\n",
    "            # üîç 3) H·ªòI THO·∫†I TH∆Ø·ªúNG\n",
    "            # =====================================================\n",
    "            messages_to_send.append(new_user_message)\n",
    "            response_text = call_llm(messages_to_send)\n",
    "\n",
    "            memory.put(new_user_message)\n",
    "            memory.put(ChatMessage(role=\"assistant\", content=response_text))\n",
    "\n",
    "            print(f\"Chatbot: {response_text}\\n\")\n",
    "\n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\\nüõë ƒê√£ d·ª´ng cu·ªôc tr√≤ chuy·ªán.\")\n",
    "            break\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå L·ªói kh√¥ng mong mu·ªën trong v√≤ng l·∫∑p chat: {e}\")\n",
    "            traceback.print_exc()\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ib1uYPA0eMgM",
    "outputId": "16f90f34-6663-45e6-9faf-c4b9327a99cc"
   },
   "outputs": [],
   "source": [
    "chat_conversation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5FJhZ_yKVeSI"
   },
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import openai\n",
    "\n",
    "\n",
    "client = openai.OpenAI(\n",
    "    api_key=os.environ.get(\"OPENAI_API_KEY\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B2W9jvFWIH05"
   },
   "outputs": [],
   "source": [
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.core import Settings\n",
    "\n",
    "# ----------------------------\n",
    "# Load GPT-4o-mini\n",
    "# ----------------------------\n",
    "llm_gptmini = OpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0.2,   # gi·∫£m ƒë·ªô s√°ng t·∫°o, t·∫≠p trung fact-based\n",
    "    max_tokens=512\n",
    ")\n",
    "\n",
    "# G√°n cho Settings ƒë·ªÉ d√πng trong ingest + transformation\n",
    "Settings.llm_gptmini = llm_gptmini\n",
    "\n",
    "# N·∫øu mu·ªën d√πng GPT cho ƒë√°nh gi√° c√¢u h·ªèi c≈©ng l·∫•y llm n√†y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2BIJ3nh8IT6i"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LV413dKOLvwx"
   },
   "outputs": [],
   "source": [
    "def generate_questions_from_nodes(nodes, max_q_per_node=1):\n",
    "    questions = []\n",
    "    for node in nodes:\n",
    "        prompt = f\"\"\"\n",
    "B·∫°n l√† gi·∫£ng vi√™n t√¢m l√Ω. D·ª±a v√†o ƒëo·∫°n vƒÉn b·∫£n DSM-5:\n",
    "\"{node.text[:1000]}\"\n",
    "\n",
    "So·∫°n {max_q_per_node} c√¢u h·ªèi t·ª± lu·∫≠n (Essay):\n",
    "- Kh√¥ng Yes/No\n",
    "- Ng·∫Øn g·ªçn, t·ªïng h·ª£p th√¥ng tin\n",
    "- Tr·∫£ v·ªÅ JSON List: [\"C√¢u h·ªèi 1\", ...]\n",
    "\"\"\"\n",
    "        resp = Settings.llm_gptmini.chat([ChatMessage(role=\"user\", content=prompt)])\n",
    "        content = resp.message.content.strip()\n",
    "\n",
    "        # Clean JSON n·∫øu c√≥ Markdown\n",
    "        if content.startswith(\"```json\"):\n",
    "            content = content.replace(\"```json\",\" \").replace(\"```\",\" \")\n",
    "        try:\n",
    "            q_list = json.loads(content)\n",
    "            for q in q_list:\n",
    "                questions.append({\"question\": q, \"reference_node_text\": node.text})\n",
    "        except:\n",
    "            print(f\"‚ö†Ô∏è Node {node.node_id} kh√¥ng parse ƒë∆∞·ª£c JSON.\")\n",
    "    return pd.DataFrame(questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E2failPEL28W"
   },
   "outputs": [],
   "source": [
    "def answer_with_llama(question, top_k=3, final_top=2):\n",
    "    \"\"\"\n",
    "    Tr·∫£ l·ªùi m·ªôt c√¢u h·ªèi theo ƒë√∫ng logic c·ªßa chat pipeline:\n",
    "    - N·∫øu c·∫ßn RAG th√¨ th√™m context + reset memory sau khi tr·∫£ l·ªùi\n",
    "    - N·∫øu kh√¥ng c·∫ßn RAG th√¨ tr·∫£ l·ªùi tr·ª±c ti·∫øp\n",
    "    - KH√îNG ghi v√†o b·ªô nh·ªõ h·ªôi tho·∫°i\n",
    "    \"\"\"\n",
    "    # Detect c·∫•m/meta\n",
    "    forbidden_meta = [\"rag\", \"retrieve\", \"rerank\", \"vector\", \"dsm context\"]\n",
    "    if any(k in question.lower() for k in forbidden_meta):\n",
    "        return \"‚ùå C√¢u h·ªèi ch·ª©a t·ª´ kh√≥a h·ªá th·ªëng ‚Äì kh√¥ng h·ª£p l·ªá.\"\n",
    "\n",
    "    # N·∫øu c·∫ßn RAG\n",
    "    if should_use_rag(question):\n",
    "        context = rag_retrieve_and_rerank(question, top_k=top_k, final_top=final_top)\n",
    "\n",
    "        system_msg = (\n",
    "            f\"--- Th√¥ng tin DSM-5 li√™n quan ---\\n{context}\\n\"\n",
    "            \"--- H·∫øt th√¥ng tin ---\"\n",
    "        )\n",
    "\n",
    "        messages = [\n",
    "            ChatMessage(role=\"system\", content=system_msg),\n",
    "            ChatMessage(role=\"user\", content=question)\n",
    "        ]\n",
    "\n",
    "        answer = call_llm(messages)\n",
    "\n",
    "        # QUY T·∫ÆC B·∫ÆT BU·ªòC C·ª¶A B·∫†N\n",
    "        reset_memory()\n",
    "\n",
    "        return answer\n",
    "\n",
    "    # C√¢u h·ªèi b√¨nh th∆∞·ªùng\n",
    "    messages = [\n",
    "        ChatMessage(role=\"system\", content=\"B·∫°n l√† tr·ª£ l√Ω t√¢m l√Ω AI.\"),\n",
    "        ChatMessage(role=\"user\", content=question)\n",
    "    ]\n",
    "    return call_llm(messages)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CzRZZ4WBprWm"
   },
   "outputs": [],
   "source": [
    "def evaluate_with_gpt(question, answer, reference_text=None):\n",
    "    \"\"\"\n",
    "    ƒê√°nh gi√° c√¢u tr·∫£ l·ªùi GPT d·ª±a tr√™n c√¢u h·ªèi v√† vƒÉn b·∫£n tham chi·∫øu (n·∫øu c√≥).\n",
    "    Tr·∫£ ƒëi·ªÉm d·∫°ng float ƒë·ªÉ ƒë√°nh gi√° m∆∞·ª£t h∆°n.\n",
    "    \"\"\"\n",
    "\n",
    "    # C·∫Øt theo chunk size n·∫øu c√≥ reference\n",
    "    ref_text_snippet = reference_text[:1000] if reference_text else None\n",
    "\n",
    "    eval_prompt = f\"\"\"\n",
    "B·∫°n l√† gi√°m kh·∫£o t√¢m l√Ω AI. H√£y ƒë√°nh gi√° c√¢u tr·∫£ l·ªùi sau:\n",
    "\n",
    "Question: \"{question}\"\n",
    "Answer: \"{answer}\"\n",
    "\"\"\"\n",
    "    if ref_text_snippet:\n",
    "        eval_prompt += f\"\"\"\n",
    "VƒÉn b·∫£n tham chi·∫øu (reference):\n",
    "\"{ref_text_snippet}\"\n",
    "\n",
    "H√£y ch·∫•m ƒëi·ªÉm d·ª±a tr√™n vi·ªác c√¢u tr·∫£ l·ªùi c√≥ ƒë√∫ng v√† trung th·ª±c v·ªõi reference_text.\n",
    "\"\"\"\n",
    "\n",
    "    eval_prompt += \"\"\"\n",
    "TI√äU CH√ç (d·∫°ng FLOAT):\n",
    "1. Correctness: 0.0‚Äì5.0\n",
    "2. Faithfulness: 0.0‚Äì1.0\n",
    "3. Relevancy: 0.0‚Äì1.0\n",
    "\n",
    "Tr·∫£ v·ªÅ JSON:\n",
    "{\"correctness\": float, \"faithfulness\": float, \"relevancy\": float}\n",
    "\"\"\"\n",
    "\n",
    "    # GPT tr·∫£ l·ªùi\n",
    "    resp = Settings.llm_gptmini.chat([ChatMessage(role=\"user\", content=eval_prompt)])\n",
    "    content = resp.message.content.strip()\n",
    "\n",
    "    # Clean JSON n·∫øu c√≥ Markdown\n",
    "    if content.startswith(\"```\"):\n",
    "        content = content.replace(\"```json\", \"\").replace(\"```\", \"\")\n",
    "\n",
    "    # Parse JSON\n",
    "    try:\n",
    "        data = json.loads(content)\n",
    "\n",
    "        # Convert t·∫•t c·∫£ sang float an to√†n\n",
    "        result = {\n",
    "            \"correctness\": float(data.get(\"correctness\", 0)),\n",
    "            \"faithfulness\": float(data.get(\"faithfulness\", 0)),\n",
    "            \"relevancy\": float(data.get(\"relevancy\", 0))\n",
    "        }\n",
    "        return result\n",
    "\n",
    "    except Exception:\n",
    "        return {\n",
    "            \"correctness\": 0.0,\n",
    "            \"faithfulness\": 0.0,\n",
    "            \"relevancy\": 0.0,\n",
    "            \"reasoning\": \"Parse error\"\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wDcCvip_L_US"
   },
   "outputs": [],
   "source": [
    "# ===============================================\n",
    "# 5Ô∏è‚É£ In ƒëi·ªÉm trung b√¨nh\n",
    "# ===============================================\n",
    "def print_average_scores(df_eval):\n",
    "    print(\"üìä ƒêi·ªÉm trung b√¨nh:\")\n",
    "    print(f\"Correctness: {df_eval['correctness'].mean():.2f} / 5\")\n",
    "    print(f\"Faithfulness: {df_eval['faithfulness'].mean():.2f} / 1\")\n",
    "    print(f\"Relevancy: {df_eval['relevancy'].mean():.2f} / 1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZWFvddH9WDXl"
   },
   "outputs": [],
   "source": [
    "# 5Ô∏è‚É£ L∆∞u k·∫øt qu·∫£ ra CSV\n",
    "# ===============================================\n",
    "def save_eval_results(df_eval, filename=\"eval_results.csv\"):\n",
    "    df_eval.to_csv(filename, index=False, encoding=\"utf-8-sig\")\n",
    "    print(f\"üíæ K·∫øt qu·∫£ ƒë√°nh gi√° ƒë√£ l∆∞u: {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7cgLlggwMBmV"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def generate_question_set(nodes, max_q_per_node=1, save_path=\"questions_full.csv\"):\n",
    "    \"\"\"\n",
    "    Sinh to√†n b·ªô c√¢u h·ªèi t·ª´ nodes v·ªõi tqdm quan s√°t ti·∫øn tr√¨nh.\n",
    "    \"\"\"\n",
    "    print(\"\\nüß© ƒêang sinh to√†n b·ªô c√¢u h·ªèi t·ª´ nodes...\")\n",
    "\n",
    "    all_questions = []\n",
    "\n",
    "    # tqdm ƒë·ªÉ theo d√µi t·ª´ng node\n",
    "    for node in tqdm(nodes, desc=\"Generating questions\"):\n",
    "        df_node = generate_questions_from_nodes(\n",
    "            [node],\n",
    "            max_q_per_node=max_q_per_node\n",
    "        )\n",
    "        all_questions.append(df_node)\n",
    "\n",
    "    # G·ªôp t·∫•t c·∫£\n",
    "    df_questions_all = pd.concat(all_questions, ignore_index=True)\n",
    "\n",
    "    print(f\"üì¶ T·ªïng c√¢u h·ªèi ƒë∆∞·ª£c sinh ra: {len(df_questions_all)}\")\n",
    "\n",
    "    # L∆∞u CSV\n",
    "    if save_path:\n",
    "        try:\n",
    "            df_questions_all.to_csv(save_path, index=False, encoding='utf-8')\n",
    "            print(f\"üíæ ƒê√£ l∆∞u to√†n b·ªô c√¢u h·ªèi t·∫°i: {save_path}\")\n",
    "        except Exception as e:\n",
    "            print(\"‚ùå L·ªói khi l∆∞u CSV:\", e)\n",
    "\n",
    "    return df_questions_all\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3f-BSLHpy1AI"
   },
   "outputs": [],
   "source": [
    "def evaluate_question_set(csv_path, max_questions=200):\n",
    "    \"\"\"\n",
    "    ƒê√°nh gi√° c√¢u h·ªèi t·ª´ CSV:\n",
    "    - Load to√†n b·ªô c√¢u h·ªèi\n",
    "    - Sample max_questions c√¢u\n",
    "    - GPT tr·∫£ l·ªùi\n",
    "    - GPT t·ª± ƒë√°nh gi√° d·ª±a reference_text\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"\\nüîé B·∫Øt ƒë·∫ßu load c√¢u h·ªèi t·ª´ CSV...\")\n",
    "\n",
    "    # 1) Load CSV\n",
    "    try:\n",
    "        df_questions_full = pd.read_csv(csv_path)\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Kh√¥ng th·ªÉ load CSV: {e}\")\n",
    "        return None\n",
    "\n",
    "    print(f\"üì¶ T·ªïng c√¢u h·ªèi trong CSV: {len(df_questions_full)}\")\n",
    "\n",
    "    # 2) Random sample l√∫c ƒë√°nh gi√°\n",
    "    if len(df_questions_full) > max_questions:\n",
    "        df_questions = (\n",
    "            df_questions_full\n",
    "            .sample(n=max_questions, random_state=42)\n",
    "            .reset_index(drop=True)\n",
    "        )\n",
    "    else:\n",
    "        df_questions = df_questions_full.copy()\n",
    "\n",
    "    print(f\"üìò S·ªë c√¢u h·ªèi ƒëem ƒëi ƒë√°nh gi√°: {len(df_questions)}\")\n",
    "\n",
    "    results = []\n",
    "\n",
    "    # 3) ƒê√°nh gi√° t·ª´ng c√¢u v·ªõi progress bar\n",
    "    for idx, row in tqdm(df_questions.iterrows(),\n",
    "                         total=len(df_questions),\n",
    "                         desc=\"Evaluating questions\"):\n",
    "        question = row[\"question\"]\n",
    "        reference_text = row[\"reference_node_text\"]\n",
    "\n",
    "        # LLAMA tr·∫£ l·ªùi\n",
    "        answer = answer_with_llama(question)\n",
    "\n",
    "        # GPT ƒë√°nh gi√°\n",
    "        eval_res = evaluate_with_gpt(\n",
    "            question=question,\n",
    "            answer=answer,\n",
    "            reference_text=reference_text\n",
    "        )\n",
    "\n",
    "        eval_res.update({\n",
    "            \"question\": question,\n",
    "            \"answer\": answer,\n",
    "            \"reference_text\": reference_text\n",
    "        })\n",
    "\n",
    "        results.append(eval_res)\n",
    "\n",
    "    # 4) Convert k·∫øt qu·∫£\n",
    "    df_result = pd.DataFrame(results)\n",
    "\n",
    "    # 5) In ƒëi·ªÉm trung b√¨nh\n",
    "    if not df_result.empty:\n",
    "        print_average_scores(df_result)\n",
    "\n",
    "    # 6) L∆∞u k·∫øt qu·∫£ ra file\n",
    "    save_eval_results(df_result)\n",
    "\n",
    "    print(\"üéâ ƒê√°nh gi√° ho√†n t·∫•t!\")\n",
    "    return df_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DUqP4FBeMFHV"
   },
   "outputs": [],
   "source": [
    "!pip install -q nest_asyncio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R8BN54xuzst2",
    "outputId": "49ad50f1-17eb-4311-f082-ca601100c1e8"
   },
   "outputs": [],
   "source": [
    "df_questions_all = generate_question_set(\n",
    "    all_nodes,\n",
    "    max_q_per_node=1,\n",
    "    save_path=\"questions_full.csv\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GGG31x4h0c7T",
    "outputId": "2823f197-c255-463b-f27c-3d670a65a77c"
   },
   "outputs": [],
   "source": [
    "df_eval = evaluate_question_set(\n",
    "    csv_path=\"/content/drive/MyDrive/DTCNTT/questions_full.csv\",\n",
    "    max_questions=200\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rspd43NfrQz9",
    "outputId": "b6237d38-29b5-4bc0-957e-1940b7fc5581"
   },
   "outputs": [],
   "source": [
    "print(\"\\nüîπ Sample k·∫øt qu·∫£:\")\n",
    "print(df_eval.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fCY6HUtXijGC",
    "outputId": "91f27204-728e-459d-f348-14773df707e9"
   },
   "outputs": [],
   "source": [
    "print_average_scores(df_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gputplZeraBJ",
    "outputId": "046065ec-bbb3-4546-cfb5-c66a3d7b8612"
   },
   "outputs": [],
   "source": [
    "# 1Ô∏è‚É£ Th√¥ng tin t·ªïng quan\n",
    "print(\"=== Th√¥ng tin DataFrame ===\")\n",
    "print(df_eval.info())\n",
    "print(\"\\n=== Th·ªëng k√™ m√¥ t·∫£ c√°c c·ªôt s·ªë ===\")\n",
    "print(df_eval.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 362
    },
    "id": "EkpzDpRYrfMQ",
    "outputId": "5cf00835-c28a-43d2-a541-96a05d854c7d"
   },
   "outputs": [],
   "source": [
    "sample_df = df_eval[['question', 'answer', 'correctness', 'faithfulness', 'relevancy']].sample(10, random_state=42)\n",
    "sample_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GPGLTHeupwZP"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "L4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
